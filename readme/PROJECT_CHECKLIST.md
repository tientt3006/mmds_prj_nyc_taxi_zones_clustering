# ‚úÖ CHECKLIST HO√ÄN TH√ÄNH D·ª∞ √ÅN

## GIAI ƒêO·∫†N 1: CHU·∫®N B·ªä H·∫† T·∫¶NG

### Setup VMware VMs
- [ ] T·∫°o VM Ubuntu 1 (Master): 6GB RAM, 2-4 cores, 100GB disk
- [ ] T·∫°o VM Ubuntu 2 (Worker): 4GB RAM, 2 cores, 80GB disk
- [ ] C·∫•u h√¨nh network (NAT ho·∫∑c Bridged)
- [ ] Ghi l·∫°i IP addresses c·ªßa c·∫£ 2 VMs
- [ ] Test ping gi·ªØa 2 VMs

### C√†i ƒë·∫∑t ph·∫ßn m·ªÅm c∆° b·∫£n
- [ ] Update OS: `sudo apt update && sudo apt upgrade`
- [ ] C√†i Java 11: `sudo apt install openjdk-11-jdk`
- [ ] C√†i Python 3: `sudo apt install python3 python3-pip`
- [ ] C√†i SSH: `sudo apt install openssh-server`
- [ ] Verify Java: `java -version`
- [ ] Verify Python: `python3 --version`

### Thi·∫øt l·∫≠p /etc/hosts
- [ ] Tr√™n Master: Th√™m `192.168.x.x master` v√† `192.168.x.x worker1`
- [ ] Tr√™n Worker: Th√™m `192.168.x.x master` v√† `192.168.x.x worker1`
- [ ] Test: `ping master`, `ping worker1`

### Thi·∫øt l·∫≠p SSH passwordless
- [ ] Tr√™n Master: `ssh-keygen -t rsa`
- [ ] Copy key: `ssh-copy-id user@master` v√† `ssh-copy-id user@worker1`
- [ ] Test: `ssh worker1` (kh√¥ng c·∫ßn password)
- [ ] Test: `ssh master` (kh√¥ng c·∫ßn password)

---

## GIAI ƒêO·∫†N 2: C√ÄI ƒê·∫∂T HADOOP

### Download v√† c√†i ƒë·∫∑t
- [ ] Download Hadoop 3.3.6
- [ ] Extract v√†o /opt/hadoop
- [ ] Set HADOOP_HOME trong ~/.bashrc
- [ ] Set PATH ƒë·ªÉ include Hadoop bins
- [ ] Source ~/.bashrc

### C·∫•u h√¨nh Hadoop
- [ ] Edit core-site.xml (fs.defaultFS)
- [ ] Edit hdfs-site.xml (replication, namenode, datanode)
- [ ] Edit yarn-site.xml (resourcemanager, nodemanager)
- [ ] Edit mapred-site.xml (framework.name)
- [ ] Edit workers file (list all nodes)
- [ ] Copy config sang worker node

### Kh·ªüi ƒë·ªông HDFS
- [ ] Format namenode: `hdfs namenode -format`
- [ ] Start HDFS: `start-dfs.sh`
- [ ] Verify v·ªõi jps (NameNode, DataNode)
- [ ] Test Web UI: http://master:9870
- [ ] Start YARN: `start-yarn.sh`
- [ ] Test YARN UI: http://master:8088

### Test HDFS
- [ ] Create test dir: `hdfs dfs -mkdir /test`
- [ ] Upload file: `hdfs dfs -put /etc/hosts /test/`
- [ ] List: `hdfs dfs -ls /test`
- [ ] Cat: `hdfs dfs -cat /test/hosts`
- [ ] Remove: `hdfs dfs -rm -r /test`

---

## GIAI ƒêO·∫†N 3: C√ÄI ƒê·∫∂T SPARK

### Download v√† c√†i ƒë·∫∑t
- [ ] Download Spark 3.5.0 (with Hadoop 3)
- [ ] Extract v√†o /opt/spark
- [ ] Set SPARK_HOME trong ~/.bashrc
- [ ] Set PATH ƒë·ªÉ include Spark bins
- [ ] Source ~/.bashrc

### C·∫•u h√¨nh Spark
- [ ] Copy spark-env.sh.template ‚Üí spark-env.sh
- [ ] Edit spark-env.sh (JAVA_HOME, HADOOP_CONF_DIR, master host)
- [ ] Copy workers.template ‚Üí workers
- [ ] Edit workers (list all nodes)
- [ ] Copy config sang worker node

### Kh·ªüi ƒë·ªông Spark
- [ ] Start Master: `start-master.sh`
- [ ] Start Workers: `start-workers.sh`
- [ ] Verify v·ªõi jps (Master, Worker)
- [ ] Test Web UI: http://master:8080

### Test Spark
- [ ] Run example: `spark-submit --class org.apache.spark.examples.SparkPi ...`
- [ ] Test PySpark: `pyspark --master spark://master:7077`
- [ ] Create simple RDD, verify output

---

## GIAI ƒêO·∫†N 4: C√ÄI ƒê·∫∂T GRAPHFRAMES

- [ ] Download GraphFrames JAR
- [ ] Copy JAR v√†o $SPARK_HOME/jars/
- [ ] Ho·∫∑c install via pip: `pip3 install graphframes`
- [ ] Test import trong PySpark

---

## GIAI ƒêO·∫†N 5: DOWNLOAD V√Ä UPLOAD D·ªÆ LI·ªÜU

### Download NYC Taxi Data
- [ ] T·∫°o th∆∞ m·ª•c: `mkdir ~/nyc_taxi_data`
- [ ] Download 2019 data (12 files): `wget ...`
- [ ] Download 2020 data (12 files): `wget ...`
- [ ] Verify 24 parquet files t·ªïng ~30GB
- [ ] Download taxi zone lookup CSV

### Upload l√™n HDFS
- [ ] Create HDFS dir: `hdfs dfs -mkdir -p /user/taxi/raw_data`
- [ ] Upload data: `hdfs dfs -put *.parquet /user/taxi/raw_data/`
- [ ] Upload lookup: `hdfs dfs -put taxi_zone_lookup.csv /user/taxi/`
- [ ] Verify: `hdfs dfs -ls /user/taxi/raw_data/`
- [ ] Check size: `hdfs dfs -du -h /user/taxi/raw_data/`

---

## GIAI ƒêO·∫†N 6: SETUP PROJECT

### Clone/Download project code
- [ ] T·∫°o th∆∞ m·ª•c project: `~/massive_data_mining`
- [ ] Copy t·∫•t c·∫£ code files v√†o
- [ ] C·∫•u tr√∫c ƒë√∫ng: src/, config/, results/, ...
- [ ] Verify all Python files exist

### C√†i ƒë·∫∑t Python dependencies
- [ ] Install requirements: `pip3 install -r requirements.txt`
- [ ] Verify imports: Test import pyspark, pandas, matplotlib

### C·∫•u h√¨nh paths
- [ ] Check spark_config.py: Verify HDFS paths
- [ ] Adjust n·∫øu c·∫ßn (HDFS namenode address)

---

## GIAI ƒêO·∫†N 7: CH·∫†Y PIPELINE

### Pre-flight check
- [ ] Run: `bash check_setup.sh`
- [ ] Fix any errors reported
- [ ] Ensure all services running

### Ch·∫°y t·ª´ng b∆∞·ªõc
- [ ] B∆∞·ªõc 1 - Build Graph: 
  ```bash
  spark-submit --master spark://master:7077 \
    --executor-memory 2g --driver-memory 2g \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/1_build_graph.py
  ```
  - [ ] Verify output in HDFS: /user/taxi/graph/edge_list
  
- [ ] B∆∞·ªõc 2 - PageRank:
  ```bash
  spark-submit --master spark://master:7077 \
    --executor-memory 2g --driver-memory 2g \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/2_pagerank.py
  ```
  - [ ] Verify output: /user/taxi/results/pagerank_scores
  
- [ ] B∆∞·ªõc 3 - Clustering:
  ```bash
  spark-submit --master spark://master:7077 \
    --executor-memory 2g --driver-memory 2g \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/3_clustering.py
  ```
  - [ ] Verify output: /user/taxi/results/community_assignments
  
- [ ] B∆∞·ªõc 4 - Visualization:
  ```bash
  python3 src/4_visualization.py
  ```
  - [ ] Verify PNG files in results/visualizations/
  
- [ ] B∆∞·ªõc 5 - Benchmark:
  ```bash
  spark-submit --master spark://master:7077 \
    --executor-memory 2g --driver-memory 2g \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/5_benchmark.py
  ```
  - [ ] Verify JSON in results/benchmarks/

### Ho·∫∑c ch·∫°y t·∫•t c·∫£
- [ ] Run: `bash run_all.sh`
- [ ] Monitor progress
- [ ] Check logs n·∫øu c√≥ l·ªói

---

## GIAI ƒêO·∫†N 8: THU TH·∫¨P K·∫æT QU·∫¢

### Download t·ª´ HDFS
- [ ] Download edge list: `hdfs dfs -get /user/taxi/graph/edge_list ./`
- [ ] Download PageRank: `hdfs dfs -get /user/taxi/results/pagerank_scores ./`
- [ ] Download communities: `hdfs dfs -get /user/taxi/results/community_assignments ./`
- [ ] Download CSV files t·ª´ HDFS

### Ki·ªÉm tra outputs
- [ ] Xem visualizations: `ls results/visualizations/`
- [ ] Xem summary: `cat results/visualizations/summary_statistics.csv`
- [ ] Xem top zones: `cat results/visualizations/top50_zones_detailed.csv`
- [ ] Xem benchmark: `cat results/benchmarks/benchmark_*.json`

### Screenshots
- [ ] Screenshot HDFS Web UI (http://master:9870)
- [ ] Screenshot Spark Web UI (http://master:8080)
- [ ] Screenshot running job (http://master:4040)
- [ ] Screenshot visualizations (charts)
- [ ] Screenshot terminal output

---

## GIAI ƒêO·∫†N 9: VI·∫æT B√ÅO C√ÅO

### N·ªôi dung b√°o c√°o
- [ ] Trang b√¨a
- [ ] Abstract
- [ ] Introduction (ƒë·∫∑t v·∫•n ƒë·ªÅ, m·ª•c ti√™u)
- [ ] Dataset v√† m√¥ h√¨nh h√≥a ƒë·ªì th·ªã
- [ ] Thu·∫≠t to√°n (PageRank, Label Propagation)
  - [ ] C√¥ng th·ª©c to√°n (LaTeX)
  - [ ] Pseudocode
- [ ] Implementation (architecture, tech stack)
- [ ] Results
  - [ ] Tables: Top zones, communities
  - [ ] Charts: Visualizations
- [ ] Benchmark v√† scalability analysis
  - [ ] Runtime comparison table
  - [ ] Speedup chart
- [ ] Discussion (insights, applications)
- [ ] Conclusion
- [ ] References

### Slides thuy·∫øt tr√¨nh
- [ ] Slide 1: Title
- [ ] Slide 2-3: Introduction
- [ ] Slide 4: Dataset
- [ ] Slide 5-6: Algorithms
- [ ] Slide 7: Architecture
- [ ] Slide 8-10: Results (v·ªõi visualizations)
- [ ] Slide 11: Benchmark
- [ ] Slide 12: Demo (optional)
- [ ] Slide 13: Conclusion

---

## GIAI ƒêO·∫†N 10: CHU·∫®N B·ªä B·∫¢O V·ªÜ

### Technical prep
- [ ] Ensure cluster still running v√† accessible
- [ ] Test demo commands tr∆∞·ªõc
- [ ] Backup k·∫øt qu·∫£ (in case cluster crashes)
- [ ] Prepare video recording (if needed)

### Practice
- [ ] Rehearse presentation (15-20 ph√∫t)
- [ ] Prepare answers cho Q&A th∆∞·ªùng g·∫∑p
- [ ] Review key metrics (c√≥ th·ªÉ b·ªã h·ªèi numbers)

### Checklist demo
- [ ] Laptop k·∫øt n·ªëi ƒë∆∞·ª£c VMs
- [ ] Web browsers v·ªõi tabs s·∫µn (HDFS, Spark UIs)
- [ ] Terminal s·∫µn SSH v√†o master
- [ ] Command history prepared
- [ ] Backup screenshots n·∫øu demo fail

---

## üìä K·∫æT QU·∫¢ MONG ƒê·ª¢I

### Metrics c·∫ßn nh·ªõ
- Dataset size: ~30GB, 200-300M trips
- S·ªë zones: 260
- S·ªë edges: 50-100M unique pairs
- PageRank iterations: 20
- Top zone PageRank: ~0.08-0.10
- S·ªë communities: 25-35
- Runtime build graph: ~1-2 gi·ªù
- Runtime PageRank: ~30-60 ph√∫t
- Speedup 2 nodes vs 1 node: ~1.5-2x

### Key insights
- Manhattan zones c√≥ PageRank cao nh·∫•t
- Airports (JFK, LaGuardia) c≈©ng r·∫•t quan tr·ªçng
- Communities map v·ªõi NYC boroughs
- Ph√¢n ph·ªëi power-law r√µ r√†ng

---

## üéØ TI√äU CH√ç ƒê√ÅNH GI√Å (D·ª∞ KI·∫æN)

- [ ] **T√≠nh massive (30%):** Ch·ª©ng minh kh√¥ng ch·∫°y ƒë∆∞·ª£c tr√™n PC, c·∫ßn cluster
- [ ] **Thu·∫≠t to√°n (25%):** Hi·ªÉu v√† implement ƒë√∫ng PageRank, clustering
- [ ] **Implementation (20%):** Code ch·∫°y ƒë∆∞·ª£c, c√≥ k·∫øt qu·∫£
- [ ] **Benchmark (15%):** ƒêo ƒë∆∞·ª£c scalability, c√≥ comparison
- [ ] **Presentation (10%):** Tr√¨nh b√†y r√µ r√†ng, tr·∫£ l·ªùi t·ªët Q&A

---

## ‚ö†Ô∏è R·ª¶I RO V√Ä MITIGATION

| R·ªßi ro | Mitigation |
|--------|------------|
| VM crash trong demo | Backup screenshots, video recording |
| HDFS/Spark kh√¥ng start | Practice restart commands, c√≥ script |
| OOM khi ch·∫°y | ƒê√£ tune memory config, test tr∆∞·ªõc |
| Download data qu√° l√¢u | B·∫Øt ƒë·∫ßu s·ªõm, c√≥ th·ªÉ d√πng subset |
| Cluster qu√° ch·∫≠m | Optimize config, reduce iterations |

---

**Status:** ‚òê Not Started | ‚óê In Progress | ‚úÖ Completed

**Last updated:** _________

**Notes:**
_____________________________________________________________________
_____________________________________________________________________
_____________________________________________________________________
