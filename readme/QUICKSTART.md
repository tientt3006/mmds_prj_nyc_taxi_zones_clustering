# ğŸš€ QUICK START GUIDE - NYC Taxi Graph Mining

## TL;DR - CÃ¡c bÆ°á»›c chÃ­nh

### 1ï¸âƒ£ Setup Cluster (láº§n Ä‘áº§u tiÃªn)

```bash
# TrÃªn cáº£ 2 mÃ¡y Ubuntu VMs
sudo apt update && sudo apt upgrade -y
sudo apt install openjdk-11-jdk python3 python3-pip openssh-server -y

# CÃ i Hadoop (xem setup_guide.md cho chi tiáº¿t)
# CÃ i Spark
# Thiáº¿t láº­p SSH passwordless
```

### 2ï¸âƒ£ Download Data

```bash
# TrÃªn master node
mkdir ~/nyc_taxi_data
cd ~/nyc_taxi_data

# Download 24 thÃ¡ng data (2019-2020)
for year in 2019 2020; do
    for month in {01..12}; do
        wget "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_${year}-${month}.parquet"
    done
done

# Upload lÃªn HDFS
hdfs dfs -mkdir -p /user/taxi/raw_data
hdfs dfs -put *.parquet /user/taxi/raw_data/
```

### 3ï¸âƒ£ Cháº¡y Pipeline

```bash
cd ~/massive_data_mining

# **QUAN TRá»ŒNG: Kiá»ƒm tra Python environment trÆ°á»›c**
bash check_python_env.sh

# CÃ¡ch 1: Cháº¡y táº¥t cáº£ cÃ¹ng lÃºc
bash run_all.sh

# CÃ¡ch 2: Cháº¡y tá»«ng bÆ°á»›c vá»›i HDFS archive
cd src

# BÆ°á»›c 1: Build graph
spark-submit --master spark://master:7077 \
    --executor-memory 500m \
    --driver-memory 500m \
    --archives hdfs://master:9000/user/taxi/python_env/mmds-venv.tar.gz#mmds-venv \
    --conf spark.pyspark.python=./mmds-venv/bin/python3 \
    --conf spark.pyspark.driver.python=python3 \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    1_build_graph.py

# BÆ°á»›c 2: PageRank
spark-submit --master spark://master:7077 \
    --executor-memory 2g \
    --driver-memory 2g \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    2_pagerank.py

# BÆ°á»›c 3: Clustering
spark-submit --master spark://master:7077 \
    --executor-memory 2g \
    --driver-memory 2g \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    3_clustering.py

# BÆ°á»›c 4: Visualization
python3 4_visualization.py

# BÆ°á»›c 5: Benchmark
spark-submit --master spark://master:7077 \
    --executor-memory 2g \
    --driver-memory 2g \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    5_benchmark.py
```

### 4ï¸âƒ£ Xem Káº¿t Quáº£

```bash
# Download káº¿t quáº£ tá»« HDFS
hdfs dfs -get /user/taxi/results/ ~/results/

# Xem visualizations
cd ~/massive_data_mining/results/visualizations/
ls -lh

# Xem top zones
cat top50_zones_detailed.csv

# Xem summary
cat summary_statistics.csv
```

---

## ğŸ“Š Web UI Ä‘á»ƒ Monitor

Má»Ÿ browser vÃ  truy cáº­p:

- **HDFS NameNode:** http://master:9870
- **Spark Master:** http://master:8080
- **YARN ResourceManager:** http://master:8088
- **Spark Application UI:** http://master:4040 (khi job Ä‘ang cháº¡y)

---

## â±ï¸ Thá»i gian Æ°á»›c tÃ­nh

Vá»›i cluster 2 nodes (10GB RAM total):

| BÆ°á»›c | Thá»i gian | Ghi chÃº |
|------|-----------|---------|
| Download data | 2-4 giá» | TÃ¹y tá»‘c Ä‘á»™ máº¡ng |
| Upload to HDFS | 30-60 phÃºt | ~30GB data |
| Build graph | 1-2 giá» | Scan toÃ n bá»™ data |
| PageRank | 30-60 phÃºt | 20 iterations |
| Clustering | 20-40 phÃºt | Label Propagation |
| Visualization | 5-10 phÃºt | Cháº¡y local |
| Benchmark | 30-60 phÃºt | Multiple runs |

**Tá»•ng cá»™ng:** ~5-8 giá» (cháº¡y tá»± Ä‘á»™ng)

---

## ğŸ”§ Troubleshooting Nhanh

### âŒ Out of Memory

```bash
# Giáº£m memory allocation
--executor-memory 1g --driver-memory 1g

# Hoáº·c tÄƒng RAM cho VMs
# Shutdown VM â†’ VMware Settings â†’ Memory â†’ TÄƒng lÃªn 6GB
```

### âŒ HDFS Connection Refused

```bash
# Restart HDFS
stop-dfs.sh
start-dfs.sh

# Kiá»ƒm tra
jps  # Pháº£i tháº¥y NameNode, DataNode
```

### âŒ Spark Workers Not Connected

```bash
# Check /etc/hosts
cat /etc/hosts  # Pháº£i cÃ³ master vÃ  worker1

# Test SSH
ssh worker1  # Pháº£i khÃ´ng cáº§n password

# Restart Spark
stop-master.sh && stop-workers.sh
start-master.sh && start-workers.sh
```

### âŒ GraphFrames Not Found

```bash
# Äáº£m báº£o dÃ¹ng --packages
spark-submit --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 ...

# Hoáº·c download JAR thá»§ cÃ´ng
wget https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar
sudo cp *.jar $SPARK_HOME/jars/
```

---

## ğŸ“‹ Pre-flight Checklist

TrÆ°á»›c khi cháº¡y pipeline, Ä‘áº£m báº£o:

- [ ] 2 VMs Ubuntu Ä‘Ã£ cÃ i Ä‘áº·t vÃ  cháº¡y
- [ ] Hadoop HDFS Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh vÃ  khá»Ÿi Ä‘á»™ng
- [ ] Spark cluster Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh vÃ  khá»Ÿi Ä‘á»™ng
- [ ] SSH passwordless giá»¯a master-worker Ä‘Ã£ setup
- [ ] /etc/hosts Ä‘Ã£ cÃ³ entries cho master vÃ  worker
- [ ] Java 11 Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t
- [ ] Python 3 vÃ  pip Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t
- [ ] Dá»¯ liá»‡u taxi Ä‘Ã£ Ä‘Æ°á»£c upload lÃªn HDFS
- [ ] GraphFrames JAR Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t

Kiá»ƒm tra nhanh:

```bash
# Check HDFS
hdfs dfs -ls /

# Check Spark
curl http://master:8080

# Check data
hdfs dfs -ls /user/taxi/raw_data/

# Check SSH
ssh worker1 "hostname"

# Check Java
java -version

# Check Python packages
pip3 list | grep pyspark
```

---

## ğŸ’¡ Tips

### Äá»ƒ test nhanh trÆ°á»›c khi cháº¡y full data:

1. **Sá»­a trong `1_build_graph.py`:**
   ```python
   # ThÃªm .sample() Ä‘á»ƒ test vá»›i subset
   df = spark.read.parquet(HDFS_RAW_DATA).sample(0.01)  # 1% data
   ```

2. **Giáº£m sá»‘ iterations trong PageRank:**
   ```python
   # Trong spark_config.py
   PAGERANK_ITERATIONS = 5  # Thay vÃ¬ 20
   ```

### Äá»ƒ monitor resources:

```bash
# TrÃªn master node
htop  # Xem CPU vÃ  RAM usage

# Xem Spark logs
tail -f $SPARK_HOME/logs/spark-*.out

# Xem HDFS disk usage
hdfs dfs -df -h
```

---

## ğŸ“ Need Help?

1. **Xem logs chi tiáº¿t:**
   ```bash
   # Spark logs
   ls $SPARK_HOME/logs/
   
   # Hadoop logs
   ls $HADOOP_HOME/logs/
   ```

2. **Check cluster health:**
   ```bash
   # HDFS health
   hdfs dfsadmin -report
   
   # YARN health
   yarn node -list
   ```

3. **Xem setup_guide.md** Ä‘á»ƒ biáº¿t chi tiáº¿t cáº¥u hÃ¬nh

4. **Xem README.md** Ä‘á»ƒ hiá»ƒu architecture vÃ  workflow

---

**ChÃºc may máº¯n vá»›i dá»± Ã¡n! ğŸ‰**
