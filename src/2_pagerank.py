"""
BÆ¯á»šC 2: TÃNH PAGERANK CHO CÃC TAXI ZONES
Sá»­ dá»¥ng GraphFrames PageRank algorithm

Input: Edge list tá»« HDFS (output cá»§a bÆ°á»›c 1)
Output: PageRank scores cho má»—i zone
"""

import sys
import os

# ThÃªm parent directory vÃ o Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, count, sum as _sum, avg

# Import tá»« config
from config.spark_config import (
    create_spark_session,
    HDFS_GRAPH_DATA,
    HDFS_RESULTS,
    PAGERANK_ITERATIONS,
    DAMPING_FACTOR
)

# Import utils
try:
    from utils import timer, print_section, print_dataframe_stats
except ImportError:
    from src.utils import timer, print_section, print_dataframe_stats


@timer
def load_edge_list(spark):
    """
    Load edge list tá»« HDFS
    
    Returns:
        Spark DataFrame vá»›i columns: src, dst, trip_count
    """
    print_section("LOAD EDGE LIST Tá»ª HDFS")
    
    edge_path = f"{HDFS_GRAPH_DATA}edge_list"
    print(f"ğŸ“‚ Äá»c dá»¯ liá»‡u tá»«: {edge_path}")
    
    try:
        edges = spark.read.parquet(edge_path)
        print("âœ… ÄÃ£ load edge list thÃ nh cÃ´ng!")
        print_dataframe_stats(edges, "Edge List")
        
        # Show sample
        print("\nğŸ“„ Sample edges:")
        edges.show(10)
        
        return edges
        
    except Exception as e:
        print(f"âŒ Lá»—i khi load edge list: {str(e)}")
        print("\nğŸ’¡ Gá»£i Ã½:")
        print("   - Äáº£m báº£o Ä‘Ã£ cháº¡y 1_build_graph.py trÆ°á»›c")
        print(f"   - Kiá»ƒm tra path tá»“n táº¡i: hdfs dfs -ls {edge_path}")
        raise


@timer
def create_graphframe(spark, edges):
    """
    Táº¡o GraphFrame tá»« edge list
    
    Args:
        spark: SparkSession
        edges: Edge DataFrame
        
    Returns:
        GraphFrame object
    """
    print_section("Táº O GRAPHFRAME")
    
    try:
        from graphframes import GraphFrame
    except ImportError:
        print("âŒ GraphFrames chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t!")
        print("ğŸ’¡ Cháº¡y: pip install graphframes")
        print("   Hoáº·c dÃ¹ng --packages trong spark-submit")
        raise
    
    print("ğŸ”¨ Táº¡o vertices tá»« edges...")
    
    # Táº¡o vertices (unique zones)
    src_vertices = edges.select(col("src").alias("id")).distinct()
    dst_vertices = edges.select(col("dst").alias("id")).distinct()
    vertices = src_vertices.union(dst_vertices).distinct()
    
    num_vertices = vertices.count()
    print(f"   - Sá»‘ vertices (zones): {num_vertices:,}")
    
    # Chuáº©n bá»‹ edges cho GraphFrame
    gf_edges = edges.select(
        col("src"),
        col("dst"),
        col("trip_count").alias("weight")
    )
    
    num_edges = gf_edges.count()
    print(f"   - Sá»‘ edges: {num_edges:,}")
    
    # Táº¡o GraphFrame
    print("\nğŸ”¨ Táº¡o GraphFrame...")
    graph = GraphFrame(vertices, gf_edges)
    
    print("âœ… GraphFrame Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng!")
    
    return graph


@timer
def run_pagerank(graph, iterations=20, reset_prob=0.15):
    """
    Cháº¡y PageRank algorithm
    
    Args:
        graph: GraphFrame
        iterations: Sá»‘ iterations (máº·c Ä‘á»‹nh 20)
        reset_prob: Reset probability (1 - damping factor)
        
    Returns:
        DataFrame vá»›i PageRank scores
    """
    print_section("CHáº Y PAGERANK ALGORITHM")
    
    print(f"âš™ï¸  Cáº¥u hÃ¬nh:")
    print(f"   - Sá»‘ iterations: {iterations}")
    print(f"   - Damping factor: {1 - reset_prob:.2f}")
    print(f"   - Reset probability: {reset_prob:.2f}")
    
    print("\nğŸš€ Báº¯t Ä‘áº§u tÃ­nh PageRank...")
    print("   (QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t 20-60 phÃºt)")
    
    try:
        # Cháº¡y PageRank
        results = graph.pageRank(
            resetProbability=reset_prob,
            maxIter=iterations
        )
        
        # Láº¥y vertices vá»›i PageRank scores
        pagerank_df = results.vertices.select(
            col("id").alias("zone_id"),
            col("pagerank")
        )
        
        # Cache káº¿t quáº£
        pagerank_df.cache()
        
        # Statistics
        total_zones = pagerank_df.count()
        total_pr = pagerank_df.agg(_sum("pagerank")).collect()[0][0]
        avg_pr = total_pr / total_zones if total_zones > 0 else 0
        
        print("\nâœ… PageRank hoÃ n thÃ nh!")
        print(f"\nğŸ“Š Thá»‘ng kÃª:")
        print(f"   - Tá»•ng sá»‘ zones: {total_zones:,}")
        print(f"   - Tá»•ng PageRank: {total_pr:.4f}")
        print(f"   - Trung bÃ¬nh PageRank: {avg_pr:.6f}")
        
        return pagerank_df
        
    except Exception as e:
        print(f"\nâŒ Lá»—i khi cháº¡y PageRank: {str(e)}")
        raise


@timer
def analyze_pagerank_results(pagerank_df):
    """
    PhÃ¢n tÃ­ch káº¿t quáº£ PageRank
    
    Args:
        pagerank_df: DataFrame vá»›i PageRank scores
    """
    print_section("PHÃ‚N TÃCH Káº¾T QUáº¢ PAGERANK")
    
    # Sort by PageRank descending
    ranked = pagerank_df.orderBy(desc("pagerank"))
    
    # Top 20 zones
    print("ğŸ† TOP 20 ZONES QUAN TRá»ŒNG NHáº¤T:")
    print("-" * 50)
    top20 = ranked.limit(20)
    top20.show(20, truncate=False)
    
    # Distribution analysis
    print("\nğŸ“Š PhÃ¢n phá»‘i PageRank:")
    pagerank_df.describe("pagerank").show()
    
    # Concentration analysis
    total_pr = pagerank_df.agg(_sum("pagerank")).collect()[0][0]
    
    top10_pr = ranked.limit(10).agg(_sum("pagerank")).collect()[0][0]
    top10_pct = (top10_pr / total_pr) * 100
    
    top20_pr = ranked.limit(20).agg(_sum("pagerank")).collect()[0][0]
    top20_pct = (top20_pr / total_pr) * 100
    
    top50_pr = ranked.limit(50).agg(_sum("pagerank")).collect()[0][0]
    top50_pct = (top50_pr / total_pr) * 100
    
    print(f"\nğŸ“ˆ PhÃ¢n tÃ­ch concentration:")
    print(f"   - Top 10 zones: {top10_pct:.2f}% total PageRank")
    print(f"   - Top 20 zones: {top20_pct:.2f}% total PageRank")
    print(f"   - Top 50 zones: {top50_pct:.2f}% total PageRank")
    
    # Power-law check
    if top10_pct > 40:
        print("\nğŸ’¡ PhÃ¢n phá»‘i PageRank cÃ³ Ä‘áº·c Ä‘iá»ƒm POWER-LAW")
        print("   â†’ Má»™t sá»‘ zones ráº¥t quan trá»ng, pháº§n lá»›n zones Ã­t quan trá»ng hÆ¡n")


@timer
def save_pagerank_results(pagerank_df, output_path):
    """
    LÆ°u káº¿t quáº£ PageRank vÃ o HDFS
    
    Args:
        pagerank_df: DataFrame vá»›i PageRank scores
        output_path: HDFS output path
    """
    print_section("LÆ¯U Káº¾T QUáº¢ PAGERANK")
    
    print(f"ğŸ’¾ LÆ°u Parquet vÃ o: {output_path}")
    
    # Save as Parquet
    pagerank_df.write \
        .mode("overwrite") \
        .parquet(output_path)
    
    print("âœ… ÄÃ£ lÆ°u Parquet format")
    
    # Also save CSV of top 100
    csv_path = output_path.replace("/results/", "/results_csv/")
    print(f"\nğŸ’¾ LÆ°u CSV top 100 vÃ o: {csv_path}")
    
    pagerank_df.orderBy(desc("pagerank")) \
        .limit(100) \
        .coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(csv_path)
    
    print("âœ… ÄÃ£ lÆ°u CSV format")


def main():
    """Main execution"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                â•‘
    â•‘        NYC TAXI GRAPH MINING - BÆ¯á»šC 2: PAGERANK               â•‘
    â•‘                                                                â•‘
    â•‘  Má»¥c tiÃªu: TÃ­nh PageRank cho cÃ¡c taxi zones                  â•‘
    â•‘  Thuáº­t toÃ¡n: GraphFrames PageRank (iterative)                â•‘
    â•‘                                                                â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Create Spark session
    spark = create_spark_session("NYC_Taxi_PageRank")
    
    try:
        # Step 1: Load edge list
        edges = load_edge_list(spark)
        
        # Step 2: Create GraphFrame
        graph = create_graphframe(spark, edges)
        
        # Step 3: Run PageRank
        pagerank_df = run_pagerank(
            graph, 
            iterations=PAGERANK_ITERATIONS,
            reset_prob=1 - DAMPING_FACTOR
        )
        
        # Step 4: Analyze results
        analyze_pagerank_results(pagerank_df)
        
        # Step 5: Save results
        output_path = f"{HDFS_RESULTS}pagerank_scores"
        save_pagerank_results(pagerank_df, output_path)
        
        print("\n" + "="*70)
        print("ğŸ‰ HOÃ€N THÃ€NH BÆ¯á»šC 2: PAGERANK")
        print("="*70)
        print(f"\nğŸ“‚ Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_path}")
        print("\nğŸ“Œ Next steps:")
        print("   1. Cháº¡y 3_clustering.py Ä‘á»ƒ phÃ¡t hiá»‡n communities")
        print("   2. Cháº¡y 4_visualization.py Ä‘á»ƒ visualize káº¿t quáº£")
        
    except Exception as e:
        print(f"\nâŒ Lá»–I: {str(e)}")
        import traceback
        traceback.print_exc()
        
    finally:
        # Stop Spark
        spark.stop()
        print("\nğŸ›‘ ÄÃ£ dá»«ng Spark session")


if __name__ == "__main__":
    main()
