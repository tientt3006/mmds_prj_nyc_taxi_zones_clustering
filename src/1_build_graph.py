"""
BÆ¯á»šC 1: XÃ‚Y Dá»°NG EDGE LIST Tá»ª NYC TAXI DATA
Sá»­ dá»¥ng Spark MapReduce Ä‘á»ƒ táº¡o Ä‘á»“ thá»‹ giao thÃ´ng

Input: NYC TLC Yellow Taxi Parquet files (~30GB)
Output: Edge list vá»›i trá»ng sá»‘ (sá»‘ chuyáº¿n taxi giá»¯a cÃ¡c zone)

ÄÃ¢y lÃ  bÆ°á»›c MASSIVE - scan toÃ n bá»™ dá»¯ liá»‡u phÃ¢n tÃ¡n
"""

import sys
import os

# ThÃªm parent directory vÃ o Python path Ä‘á»ƒ import config
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum as _sum, avg

# Import tá»« config directory
from config.spark_config import (
    create_spark_session, 
    HDFS_RAW_DATA, 
    HDFS_GRAPH_DATA,
    PICKUP_LOCATION,
    DROPOFF_LOCATION,
    FARE_AMOUNT,
    TIP_AMOUNT,
    TRIP_DISTANCE
)

# Import tá»« src directory
try:
    from utils import timer, print_section, print_dataframe_stats, save_dataframe_as_csv
except ImportError:
    # Náº¿u import trá»±c tiáº¿p khÃ´ng Ä‘Æ°á»£c, thá»­ import tá»« src
    from src.utils import timer, print_section, print_dataframe_stats, save_dataframe_as_csv


@timer
def load_taxi_data(spark, sample_months=None):
    """
    Load dá»¯ liá»‡u taxi tá»« HDFS
    
    Args:
        spark: SparkSession
        sample_months: List cÃ¡c thÃ¡ng Ä‘á»ƒ load (None = all)
        
    Returns:
        Spark DataFrame
    """
    print_section("LOAD Dá»® LIá»†U Tá»ª HDFS")
    
    try:
        # Load táº¥t cáº£ parquet files
        print(f"ğŸ“‚ Äá»c dá»¯ liá»‡u tá»«: {HDFS_RAW_DATA}")
        df = spark.read.parquet(HDFS_RAW_DATA)
        
        print(f"âœ… ÄÃ£ load dá»¯ liá»‡u thÃ nh cÃ´ng!")
        print_dataframe_stats(df, "Raw Taxi Data")
        
        # Hiá»ƒn thá»‹ schema
        print("\nğŸ“‹ Schema:")
        df.printSchema()
        
        # Hiá»ƒn thá»‹ sample
        print("\nğŸ“„ Sample data:")
        df.show(5)
        
        return df
        
    except Exception as e:
        print(f"âŒ Lá»—i khi load dá»¯ liá»‡u: {str(e)}")
        print("\nğŸ’¡ Gá»£i Ã½:")
        print("   - Kiá»ƒm tra HDFS Ä‘Ã£ cháº¡y: hdfs dfs -ls /")
        print("   - Kiá»ƒm tra path: hdfs dfs -ls /user/taxi/raw_data/")
        print("   - Äáº£m báº£o Ä‘Ã£ upload dá»¯ liá»‡u vÃ o HDFS")
        raise


@timer
def clean_and_filter_data(df):
    """
    LÃ m sáº¡ch vÃ  lá»c dá»¯ liá»‡u
    
    Args:
        df: Raw DataFrame
        
    Returns:
        Cleaned DataFrame
    """
    print_section("LÃ€M Sáº CH Dá»® LIá»†U")
    
    print("ğŸ§¹ Ãp dá»¥ng cÃ¡c filter:")
    print("   - Loáº¡i bá» NULL location IDs")
    print("   - Loáº¡i bá» location IDs khÃ´ng há»£p lá»‡ (< 1 hoáº·c > 263)")
    print("   - Loáº¡i bá» self-loops (PU == DO)")
    print("   - Loáº¡i bá» fare Ã¢m")
    
    # Original count
    original_count = df.count()
    print(f"\nğŸ“Š Sá»‘ dÃ²ng ban Ä‘áº§u: {original_count:,}")
    
    # Cleaning pipeline
    cleaned_df = df.filter(
        (col(PICKUP_LOCATION).isNotNull()) &
        (col(DROPOFF_LOCATION).isNotNull()) &
        (col(PICKUP_LOCATION) >= 1) &
        (col(PICKUP_LOCATION) <= 263) &
        (col(DROPOFF_LOCATION) >= 1) &
        (col(DROPOFF_LOCATION) <= 263) &
        (col(PICKUP_LOCATION) != col(DROPOFF_LOCATION)) &
        (col(FARE_AMOUNT) >= 0)
    )
    
    # Cache vÃ¬ sáº½ dÃ¹ng nhiá»u láº§n
    cleaned_df.cache()
    
    cleaned_count = cleaned_df.count()
    removed = original_count - cleaned_count
    removed_pct = (removed / original_count) * 100
    
    print(f"âœ… Sá»‘ dÃ²ng sau khi lÃ m sáº¡ch: {cleaned_count:,}")
    print(f"ğŸ—‘ï¸  ÄÃ£ loáº¡i bá»: {removed:,} dÃ²ng ({removed_pct:.2f}%)")
    
    return cleaned_df


@timer
def build_edge_list(df):
    """
    XÃ¢y dá»±ng edge list tá»« trip data
    MapReduce aggregation
    
    Args:
        df: Cleaned trip DataFrame
        
    Returns:
        Edge DataFrame vá»›i columns: src, dst, trip_count, total_fare, avg_fare, total_tip
    """
    print_section("XÃ‚Y Dá»°NG EDGE LIST (MapReduce)")
    
    print("ğŸ”¨ Thá»±c hiá»‡n aggregation:")
    print("   - Group by (PULocationID, DOLocationID)")
    print("   - Count sá»‘ chuyáº¿n")
    print("   - Sum vÃ  avg cÃ¡c metrics")
    
    # MapReduce aggregation
    edge_list = df.groupBy(
        col(PICKUP_LOCATION).alias("src"),
        col(DROPOFF_LOCATION).alias("dst")
    ).agg(
        count("*").alias("trip_count"),
        _sum(FARE_AMOUNT).alias("total_fare"),
        avg(FARE_AMOUNT).alias("avg_fare"),
        _sum(TIP_AMOUNT).alias("total_tip"),
        avg(TRIP_DISTANCE).alias("avg_distance")
    )
    
    # Sort by trip count descending
    edge_list = edge_list.orderBy(col("trip_count").desc())
    
    # Cache result
    edge_list.cache()
    
    print_dataframe_stats(edge_list, "Edge List")
    
    # Show top edges
    print("\nğŸ” Top 10 cáº¡nh báº­n rá»™n nháº¥t:")
    edge_list.show(10, truncate=False)
    
    # Statistics
    total_edges = edge_list.count()
    total_trips = edge_list.agg(_sum("trip_count")).collect()[0][0]
    avg_trips_per_edge = total_trips / total_edges
    
    print(f"\nğŸ“ˆ Thá»‘ng kÃª Ä‘á»“ thá»‹:")
    print(f"   - Tá»•ng sá»‘ edges: {total_edges:,}")
    print(f"   - Tá»•ng sá»‘ trips: {total_trips:,}")
    print(f"   - Trung bÃ¬nh trips/edge: {avg_trips_per_edge:.2f}")
    
    return edge_list


@timer
def analyze_graph_structure(edge_list):
    """
    PhÃ¢n tÃ­ch cáº¥u trÃºc Ä‘á»“ thá»‹ cÆ¡ báº£n
    
    Args:
        edge_list: Edge DataFrame
    """
    print_section("PHÃ‚N TÃCH Cáº¤U TRÃšC Äá»’ THá»Š")
    
    # Unique nodes (zones)
    print("ğŸ” Äáº¿m sá»‘ nodes...")
    src_nodes = edge_list.select("src").distinct()
    dst_nodes = edge_list.select("dst").distinct()
    all_nodes = src_nodes.union(dst_nodes).distinct()
    num_nodes = all_nodes.count()
    
    print(f"   - Sá»‘ nodes (zones): {num_nodes}")
    
    # Out-degree distribution
    print("\nğŸ“Š Out-degree distribution (sá»‘ zone mÃ  má»—i zone Ä‘i Ä‘áº¿n):")
    out_degree = edge_list.groupBy("src") \
        .agg(count("dst").alias("out_degree")) \
        .orderBy(col("out_degree").desc())
    
    out_degree.describe("out_degree").show()
    
    print("\nğŸ” Top 10 zones cÃ³ out-degree cao nháº¥t:")
    out_degree.show(10)
    
    # In-degree distribution
    print("\nğŸ“Š In-degree distribution (sá»‘ zone Ä‘i Ä‘áº¿n má»—i zone):")
    in_degree = edge_list.groupBy("dst") \
        .agg(count("src").alias("in_degree")) \
        .orderBy(col("in_degree").desc())
    
    in_degree.describe("in_degree").show()
    
    print("\nğŸ” Top 10 zones cÃ³ in-degree cao nháº¥t:")
    in_degree.show(10)
    
    # Edge weight distribution
    print("\nğŸ“Š Edge weight (trip count) distribution:")
    edge_list.describe("trip_count").show()
    
    # Heavy edges
    print("\nâš–ï¸  PhÃ¢n tÃ­ch heavy edges:")
    total_trips = edge_list.agg(_sum("trip_count")).collect()[0][0]
    
    heavy_edges = edge_list.filter(col("trip_count") >= 1000)
    heavy_trips = heavy_edges.agg(_sum("trip_count")).collect()[0][0]
    heavy_pct = (heavy_trips / total_trips) * 100
    
    print(f"   - Edges vá»›i â‰¥1000 trips: {heavy_edges.count():,}")
    print(f"   - % trips trong heavy edges: {heavy_pct:.2f}%")


@timer
def save_edge_list(edge_list, output_path):
    """
    LÆ°u edge list vÃ o HDFS
    
    Args:
        edge_list: Edge DataFrame
        output_path: HDFS output path
    """
    print_section("LÆ¯U EDGE LIST")
    
    print(f"ğŸ’¾ LÆ°u vÃ o HDFS: {output_path}")
    
    # Save as Parquet (efficient for Spark)
    edge_list.write \
        .mode("overwrite") \
        .parquet(output_path)
    
    print(f"âœ… ÄÃ£ lÆ°u edge list (Parquet format)")
    
    # Also save as CSV for inspection
    csv_path = output_path.replace("/graph/", "/graph_csv/")
    print(f"\nğŸ’¾ LÆ°u CSV cho inspection: {csv_path}")
    
    edge_list.coalesce(1).write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(csv_path)
    
    print(f"âœ… ÄÃ£ lÆ°u edge list (CSV format)")


def main():
    """Main execution"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                â•‘
    â•‘        NYC TAXI GRAPH MINING - BÆ¯á»šC 1: BUILD EDGE LIST        â•‘
    â•‘                                                                â•‘
    â•‘  Má»¥c tiÃªu: XÃ¢y dá»±ng Ä‘á»“ thá»‹ giao thÃ´ng tá»« 30GB trip data      â•‘
    â•‘  PhÆ°Æ¡ng phÃ¡p: Spark MapReduce phÃ¢n tÃ¡n                        â•‘
    â•‘                                                                â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Create Spark session
    spark = create_spark_session("NYC_Taxi_Build_Graph")
    
    try:
        # Step 1: Load data
        raw_df = load_taxi_data(spark)
        
        # Step 2: Clean data
        cleaned_df = clean_and_filter_data(raw_df)
        
        # Step 3: Build edge list
        edge_list = build_edge_list(cleaned_df)
        
        # Step 4: Analyze graph
        analyze_graph_structure(edge_list)
        
        # Step 5: Save results
        output_path = f"{HDFS_GRAPH_DATA}edge_list"
        save_edge_list(edge_list, output_path)
        
        print("\n" + "="*70)
        print("ğŸ‰ HOÃ€N THÃ€NH BÆ¯á»šC 1: BUILD EDGE LIST")
        print("="*70)
        print(f"\nğŸ“‚ Edge list Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_path}")
        print("\nğŸ“Œ Next steps:")
        print("   1. Cháº¡y 2_pagerank.py Ä‘á»ƒ tÃ­nh PageRank scores")
        print("   2. Cháº¡y 3_clustering.py Ä‘á»ƒ phÃ¡t hiá»‡n communities")
        print("   3. Cháº¡y 4_visualization.py Ä‘á»ƒ visualize káº¿t quáº£")
        
    except Exception as e:
        print(f"\nâŒ Lá»–I: {str(e)}")
        import traceback
        traceback.print_exc()
        
    finally:
        # Stop Spark
        spark.stop()
        print("\nğŸ›‘ ÄÃ£ dá»«ng Spark session")


if __name__ == "__main__":
    main()
