"""
BÆ¯á»šC 3: GRAPH CLUSTERING - COMMUNITY DETECTION
Sá»­ dá»¥ng Label Propagation Algorithm Ä‘á»ƒ phÃ¡t hiá»‡n communities

Input: Edge list vÃ  PageRank scores tá»« HDFS
Output: Community assignments cho má»—i zone
"""

import sys
import os

# ThÃªm parent directory vÃ o Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum as _sum, avg, desc

# Import tá»« config
from config.spark_config import (
    create_spark_session,
    HDFS_GRAPH_DATA,
    HDFS_RESULTS
)

# Import utils
try:
    from utils import timer, print_section, print_dataframe_stats
except ImportError:
    from src.utils import timer, print_section, print_dataframe_stats


@timer
def load_graph_data(spark):
    """
    Load edge list vÃ  PageRank scores tá»« HDFS
    
    Returns:
        tuple: (edges DataFrame, pagerank DataFrame)
    """
    print_section("LOAD GRAPH DATA Tá»ª HDFS")
    
    # Load edge list
    edge_path = f"{HDFS_GRAPH_DATA}edge_list"
    print(f"ğŸ“‚ Äá»c edge list tá»«: {edge_path}")
    edges = spark.read.parquet(edge_path)
    print("âœ… ÄÃ£ load edge list")
    print_dataframe_stats(edges, "Edge List")
    
    # Load PageRank scores
    pr_path = f"{HDFS_RESULTS}pagerank_scores"
    print(f"\nğŸ“‚ Äá»c PageRank scores tá»«: {pr_path}")
    pagerank = spark.read.parquet(pr_path)
    print("âœ… ÄÃ£ load PageRank scores")
    print_dataframe_stats(pagerank, "PageRank Scores")
    
    return edges, pagerank


@timer
def create_graphframe(spark, edges):
    """
    Táº¡o GraphFrame tá»« edge list
    
    Args:
        spark: SparkSession
        edges: Edge DataFrame
        
    Returns:
        GraphFrame object
    """
    print_section("Táº O GRAPHFRAME CHO CLUSTERING")
    
    try:
        from graphframes import GraphFrame
    except ImportError:
        print("âŒ GraphFrames chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t!")
        raise
    
    # Táº¡o vertices (unique zones)
    print("ğŸ”¨ Táº¡o vertices...")
    src_vertices = edges.select(col("src").alias("id")).distinct()
    dst_vertices = edges.select(col("dst").alias("id")).distinct()
    vertices = src_vertices.union(dst_vertices).distinct()
    
    num_vertices = vertices.count()
    print(f"   - Sá»‘ vertices: {num_vertices:,}")
    
    # Prepare edges
    gf_edges = edges.select(
        col("src"),
        col("dst"),
        col("trip_count").alias("weight")
    )
    
    num_edges = gf_edges.count()
    print(f"   - Sá»‘ edges: {num_edges:,}")
    
    # Create GraphFrame
    print("\nğŸ”¨ Táº¡o GraphFrame...")
    graph = GraphFrame(vertices, gf_edges)
    
    print("âœ… GraphFrame Ä‘Ã£ Ä‘Æ°á»£c táº¡o!")
    
    return graph


@timer
def run_label_propagation(graph, max_iterations=10):
    """
    Cháº¡y Label Propagation Algorithm Ä‘á»ƒ phÃ¡t hiá»‡n communities
    
    Args:
        graph: GraphFrame
        max_iterations: Sá»‘ iterations tá»‘i Ä‘a
        
    Returns:
        DataFrame vá»›i community assignments
    """
    print_section("CHáº Y LABEL PROPAGATION ALGORITHM")
    
    print(f"âš™ï¸  Cáº¥u hÃ¬nh:")
    print(f"   - Max iterations: {max_iterations}")
    
    print("\nğŸš€ Báº¯t Ä‘áº§u Label Propagation...")
    print("   (QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t 20-40 phÃºt)")
    
    try:
        # Run Label Propagation
        result = graph.labelPropagation(maxIter=max_iterations)
        
        # Get community assignments
        communities = result.select(
            col("id").alias("zone_id"),
            col("label").alias("community_id")
        )
        
        # Cache result
        communities.cache()
        
        print("\nâœ… Label Propagation hoÃ n thÃ nh!")
        
        return communities
        
    except Exception as e:
        print(f"\nâŒ Lá»—i khi cháº¡y Label Propagation: {str(e)}")
        raise


@timer
def analyze_communities(communities, pagerank_df):
    """
    PhÃ¢n tÃ­ch communities Ä‘Æ°á»£c phÃ¡t hiá»‡n
    
    Args:
        communities: DataFrame vá»›i community assignments
        pagerank_df: DataFrame vá»›i PageRank scores
    """
    print_section("PHÃ‚N TÃCH COMMUNITIES")
    
    # Join vá»›i PageRank scores
    comm_with_pr = communities.join(
        pagerank_df,
        communities.zone_id == pagerank_df.zone_id,
        "inner"
    ).select(
        communities.zone_id,
        communities.community_id,
        pagerank_df.pagerank
    )
    
    # Community statistics
    print("ğŸ“Š Thá»‘ng kÃª communities:")
    
    comm_stats = comm_with_pr.groupBy("community_id").agg(
        count("zone_id").alias("num_zones"),
        _sum("pagerank").alias("total_pagerank"),
        avg("pagerank").alias("avg_pagerank")
    ).orderBy(desc("num_zones"))
    
    total_communities = comm_stats.count()
    print(f"   - Tá»•ng sá»‘ communities: {total_communities:,}")
    
    # Cache stats
    comm_stats.cache()
    
    # Show top communities by size
    print("\nğŸ” TOP 20 COMMUNITIES (theo sá»‘ zones):")
    print("-" * 70)
    comm_stats.show(20, truncate=False)
    
    # Distribution analysis
    print("\nğŸ“ˆ PhÃ¢n phá»‘i community size:")
    comm_stats.describe("num_zones").show()
    
    # Largest and smallest communities
    largest = comm_stats.first()
    smallest = comm_stats.orderBy("num_zones").first()
    
    print(f"\nğŸ“Œ Community lá»›n nháº¥t:")
    print(f"   - ID: {largest['community_id']}")
    print(f"   - Sá»‘ zones: {largest['num_zones']}")
    print(f"   - Total PageRank: {largest['total_pagerank']:.4f}")
    
    print(f"\nğŸ“Œ Community nhá» nháº¥t:")
    print(f"   - ID: {smallest['community_id']}")
    print(f"   - Sá»‘ zones: {smallest['num_zones']}")
    print(f"   - Total PageRank: {smallest['total_pagerank']:.4f}")
    
    # PageRank concentration in top communities
    total_pr = comm_stats.agg(_sum("total_pagerank")).collect()[0][0]
    
    top5_pr = comm_stats.limit(5).agg(_sum("total_pagerank")).collect()[0][0]
    top5_pct = (top5_pr / total_pr) * 100
    
    top10_pr = comm_stats.limit(10).agg(_sum("total_pagerank")).collect()[0][0]
    top10_pct = (top10_pr / total_pr) * 100
    
    print(f"\nğŸ“Š PageRank concentration:")
    print(f"   - Top 5 communities: {top5_pct:.2f}% total PageRank")
    print(f"   - Top 10 communities: {top10_pct:.2f}% total PageRank")
    
    return comm_stats


@timer
def analyze_community_connectivity(communities, edges):
    """
    PhÃ¢n tÃ­ch connectivity trong vÃ  giá»¯a cÃ¡c communities
    
    Args:
        communities: DataFrame vá»›i community assignments
        edges: Edge DataFrame
    """
    print_section("PHÃ‚N TÃCH CONNECTIVITY")
    
    # Join edges vá»›i community info
    edges_with_comm = edges.alias("e") \
        .join(communities.alias("c1"), col("e.src") == col("c1.zone_id")) \
        .join(communities.alias("c2"), col("e.dst") == col("c2.zone_id")) \
        .select(
            col("e.src"),
            col("e.dst"),
            col("e.trip_count"),
            col("c1.community_id").alias("src_community"),
            col("c2.community_id").alias("dst_community")
        )
    
    # Intra-community vs inter-community edges
    total_trips_result = edges_with_comm.agg(_sum("trip_count")).collect()[0][0]
    
    # Handle None case (no data)
    if total_trips_result is None:
        print("âš ï¸  KhÃ´ng cÃ³ dá»¯ liá»‡u trips Ä‘á»ƒ phÃ¢n tÃ­ch connectivity")
        return
    
    total_trips = total_trips_result
    
    intra_comm = edges_with_comm.filter(
        col("src_community") == col("dst_community")
    )
    intra_trips_result = intra_comm.agg(_sum("trip_count")).collect()[0][0]
    intra_trips = intra_trips_result if intra_trips_result is not None else 0
    intra_pct = (intra_trips / total_trips) * 100 if total_trips > 0 else 0
    
    inter_comm = edges_with_comm.filter(
        col("src_community") != col("dst_community")
    )
    inter_trips_result = inter_comm.agg(_sum("trip_count")).collect()[0][0]
    inter_trips = inter_trips_result if inter_trips_result is not None else 0
    inter_pct = (inter_trips / total_trips) * 100 if total_trips > 0 else 0
    
    print(f"ğŸ”— Edge connectivity:")
    print(f"   - Intra-community trips: {intra_trips:,} ({intra_pct:.2f}%)")
    print(f"   - Inter-community trips: {inter_trips:,} ({inter_pct:.2f}%)")
    
    if intra_pct > 50:
        print("\nğŸ’¡ Communities cÃ³ tÃ­nh cháº¥t strong intra-connectivity")
        print("   â†’ Zones trong cÃ¹ng community cÃ³ traffic ná»™i bá»™ cao")
    elif inter_pct > 50:
        print("\nğŸ’¡ Communities cÃ³ tÃ­nh cháº¥t weak intra-connectivity")
        print("   â†’ Traffic giá»¯a cÃ¡c communities cao hÆ¡n traffic ná»™i bá»™")
    else:
        print("\nğŸ’¡ Communities cÃ³ connectivity cÃ¢n báº±ng")


@timer
def save_results(communities, comm_stats, output_path):
    """
    LÆ°u káº¿t quáº£ clustering vÃ o HDFS
    
    Args:
        communities: DataFrame vá»›i community assignments
        comm_stats: DataFrame vá»›i community statistics
        output_path: HDFS output path
    """
    print_section("LÆ¯U Káº¾T QUáº¢ CLUSTERING")
    
    # Save community assignments (Parquet)
    assign_path = f"{output_path}/community_assignments"
    print(f"ğŸ’¾ LÆ°u assignments vÃ o: {assign_path}")
    
    communities.write \
        .mode("overwrite") \
        .parquet(assign_path)
    
    print("âœ… ÄÃ£ lÆ°u community assignments (Parquet)")
    
    # Save community statistics (Parquet)
    stats_path = f"{output_path}/community_statistics"
    print(f"\nğŸ’¾ LÆ°u statistics vÃ o: {stats_path}")
    
    comm_stats.write \
        .mode("overwrite") \
        .parquet(stats_path)
    
    print("âœ… ÄÃ£ lÆ°u community statistics (Parquet)")
    
    # Also save CSV for inspection
    csv_path = f"{output_path}/community_assignments_csv"
    print(f"\nğŸ’¾ LÆ°u CSV vÃ o: {csv_path}")
    
    communities.coalesce(1).write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(csv_path)
    
    print("âœ… ÄÃ£ lÆ°u CSV format")


def main():
    """Main execution"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                â•‘
    â•‘      NYC TAXI GRAPH MINING - BÆ¯á»šC 3: GRAPH CLUSTERING         â•‘
    â•‘                                                                â•‘
    â•‘  Má»¥c tiÃªu: PhÃ¡t hiá»‡n communities trong Ä‘á»“ thá»‹ giao thÃ´ng     â•‘
    â•‘  Thuáº­t toÃ¡n: Label Propagation                                â•‘
    â•‘                                                                â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Create Spark session
    spark = create_spark_session("NYC_Taxi_Clustering")
    
    try:
        # Step 1: Load data
        edges, pagerank_df = load_graph_data(spark)
        
        # Step 2: Create GraphFrame
        graph = create_graphframe(spark, edges)
        
        # Step 3: Run Label Propagation
        communities = run_label_propagation(graph, max_iterations=10)
        
        # Step 4: Analyze communities
        comm_stats = analyze_communities(communities, pagerank_df)
        
        # Step 5: Analyze connectivity
        analyze_community_connectivity(communities, edges)
        
        # Step 6: Save results
        output_path = f"{HDFS_RESULTS}clustering"
        save_results(communities, comm_stats, output_path)
        
        print("\n" + "="*70)
        print("ğŸ‰ HOÃ€N THÃ€NH BÆ¯á»šC 3: GRAPH CLUSTERING")
        print("="*70)
        print(f"\nğŸ“‚ Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_path}")
        print("\nğŸ“Œ Next steps:")
        print("   1. Cháº¡y 4_visualization.py Ä‘á»ƒ visualize káº¿t quáº£")
        print("   2. Cháº¡y 5_benchmark.py Ä‘á»ƒ Ä‘o scalability")
        
    except Exception as e:
        print(f"\nâŒ Lá»–I: {str(e)}")
        import traceback
        traceback.print_exc()
        
    finally:
        # Stop Spark
        spark.stop()
        print("\nğŸ›‘ ÄÃ£ dá»«ng Spark session")


if __name__ == "__main__":
    main()
