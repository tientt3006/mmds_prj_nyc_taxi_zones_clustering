"""
B∆Ø·ªöC 3: GRAPH CLUSTERING - PH√ÅT HI·ªÜN COMMUNITIES
S·ª≠ d·ª•ng Label Propagation Algorithm ƒë·ªÉ t√¨m clusters c·ªßa taxi zones

Input: Graph t·ª´ b∆∞·ªõc 1
Output: Communities (clusters) c·ªßa zones

Ph√°t hi·ªán c√°c khu v·ª±c c√≥ giao th√¥ng n·ªôi b·ªô ch·∫∑t ch·∫Ω
"""

import sys
sys.path.append('../config')

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, desc, avg
from graphframes import GraphFrame
from spark_config import (
    create_spark_session,
    HDFS_GRAPH_DATA,
    HDFS_RESULTS
)
from utils import timer, print_section, print_dataframe_stats


@timer
def load_graph_data(spark):
    """
    Load edge list v√† PageRank results
    
    Args:
        spark: SparkSession
        
    Returns:
        tuple (edges_df, pagerank_df)
    """
    print_section("LOAD GRAPH DATA")
    
    # Load edges
    edge_path = f"{HDFS_GRAPH_DATA}edge_list"
    print(f"üìÇ Load edges: {edge_path}")
    edges_df = spark.read.parquet(edge_path)
    print_dataframe_stats(edges_df, "Edges")
    
    # Load PageRank
    pr_path = f"{HDFS_RESULTS}/pagerank_scores"
    print(f"\nüìÇ Load PageRank: {pr_path}")
    pagerank_df = spark.read.parquet(pr_path)
    print_dataframe_stats(pagerank_df, "PageRank")
    
    return edges_df, pagerank_df


@timer
def create_graph_with_pagerank(edges_df, pagerank_df):
    """
    T·∫°o GraphFrame v·ªõi th√¥ng tin PageRank
    
    Args:
        edges_df: Edge DataFrame
        pagerank_df: PageRank DataFrame
        
    Returns:
        GraphFrame
    """
    print_section("T·∫†O GRAPHFRAME V·ªöI PAGERANK")
    
    # Rename columns for GraphFrame
    vertices = pagerank_df.select(
        col("zone_id").alias("id"),
        col("pagerank")
    )
    
    gf_edges = edges_df.select(
        col("src"),
        col("dst"),
        col("trip_count").alias("weight")
    )
    
    graph = GraphFrame(vertices, gf_edges)
    
    print(f"‚úÖ GraphFrame created:")
    print(f"   - Vertices: {vertices.count()}")
    print(f"   - Edges: {gf_edges.count()}")
    
    return graph


@timer
def run_label_propagation(graph, max_iter=10):
    """
    Ch·∫°y Label Propagation Algorithm
    
    Args:
        graph: GraphFrame
        max_iter: S·ªë iteration t·ªëi ƒëa
        
    Returns:
        DataFrame v·ªõi label assignments
    """
    print_section(f"LABEL PROPAGATION ALGORITHM ({max_iter} iterations)")
    
    print("üöÄ Ch·∫°y Label Propagation...")
    print("   Thu·∫≠t to√°n n√†y t·ª± ƒë·ªông ph√°t hi·ªán communities trong graph")
    print("   C√°c node trong c√πng community c√≥ k·∫øt n·ªëi ch·∫∑t ch·∫Ω v·ªõi nhau")
    
    # Run Label Propagation
    result = graph.labelPropagation(maxIter=max_iter)
    
    communities = result.select(
        col("id").alias("zone_id"),
        col("label").alias("community_id"),
        col("pagerank")
    )
    
    print("\n‚úÖ Label Propagation ho√†n th√†nh!")
    
    return communities


@timer
def analyze_communities(communities_df):
    """
    Ph√¢n t√≠ch c√°c communities ƒë∆∞·ª£c ph√°t hi·ªán
    
    Args:
        communities_df: DataFrame v·ªõi community assignments
    """
    print_section("PH√ÇN T√çCH COMMUNITIES")
    
    # Count communities
    num_communities = communities_df.select("community_id").distinct().count()
    print(f"üîç S·ªë communities ph√°t hi·ªán: {num_communities}")
    
    # Community size distribution
    print("\nüìä Ph√¢n b·ªë k√≠ch th∆∞·ªõc communities:")
    
    community_sizes = communities_df.groupBy("community_id") \
        .agg(
            count("zone_id").alias("size"),
            avg("pagerank").alias("avg_pagerank")
        ) \
        .orderBy(desc("size"))
    
    community_sizes.describe("size").show()
    
    # Top communities
    print("\nüèÜ TOP 20 COMMUNITIES L·ªöN NH·∫§T:")
    community_sizes.show(20, truncate=False)
    
    # Small communities
    small_communities = community_sizes.filter(col("size") <= 3)
    num_small = small_communities.count()
    print(f"\nüîç S·ªë communities nh·ªè (‚â§3 zones): {num_small}")
    
    # Large communities (‚â•10 zones)
    large_communities = community_sizes.filter(col("size") >= 10)
    num_large = large_communities.count()
    print(f"üîç S·ªë communities l·ªõn (‚â•10 zones): {num_large}")
    
    return community_sizes


@timer
def analyze_community_details(communities_df, edges_df, top_n=5):
    """
    Ph√¢n t√≠ch chi ti·∫øt c√°c communities l·ªõn nh·∫•t
    
    Args:
        communities_df: Community assignments
        edges_df: Edge list
        top_n: S·ªë communities l·ªõn nh·∫•t ƒë·ªÉ ph√¢n t√≠ch
    """
    print_section(f"PH√ÇN T√çCH CHI TI·∫æT TOP {top_n} COMMUNITIES")
    
    # Get top communities
    top_communities = communities_df.groupBy("community_id") \
        .agg(count("zone_id").alias("size")) \
        .orderBy(desc("size")) \
        .limit(top_n) \
        .select("community_id") \
        .rdd.flatMap(lambda x: x).collect()
    
    for i, comm_id in enumerate(top_communities, 1):
        print(f"\n{'='*60}")
        print(f"üìç COMMUNITY #{i} (ID: {comm_id})")
        print(f"{'='*60}")
        
        # Get zones in this community
        comm_zones = communities_df.filter(col("community_id") == comm_id)
        
        print(f"\nüèôÔ∏è  Zones trong community:")
        comm_zones.orderBy(desc("pagerank")).show(20, truncate=False)
        
        # Internal edges (edges within community)
        zone_list = comm_zones.select("zone_id").rdd.flatMap(lambda x: x).collect()
        
        internal_edges = edges_df.filter(
            (col("src").isin(zone_list)) & 
            (col("dst").isin(zone_list))
        )
        
        # External edges
        external_edges_out = edges_df.filter(
            (col("src").isin(zone_list)) & 
            (~col("dst").isin(zone_list))
        )
        
        external_edges_in = edges_df.filter(
            (~col("src").isin(zone_list)) & 
            (col("dst").isin(zone_list))
        )
        
        from pyspark.sql.functions import sum as _sum
        
        internal_trips = internal_edges.agg(_sum("trip_count")).collect()[0][0] or 0
        external_trips_out = external_edges_out.agg(_sum("trip_count")).collect()[0][0] or 0
        external_trips_in = external_edges_in.agg(_sum("trip_count")).collect()[0][0] or 0
        
        total_trips = internal_trips + external_trips_out + external_trips_in
        internal_ratio = (internal_trips / total_trips * 100) if total_trips > 0 else 0
        
        print(f"\nüìä Traffic Statistics:")
        print(f"   - Internal trips: {internal_trips:,} ({internal_ratio:.2f}%)")
        print(f"   - External trips (out): {external_trips_out:,}")
        print(f"   - External trips (in): {external_trips_in:,}")
        
        # Modularity-like metric
        if internal_ratio > 50:
            print(f"   ‚úÖ ƒê√¢y l√† community ch·∫∑t ch·∫Ω (>50% internal traffic)")
        else:
            print(f"   ‚ö†Ô∏è  Community l·ªèng l·∫ªo (<50% internal traffic)")


@timer
def save_clustering_results(communities_df, community_sizes, output_path):
    """
    L∆∞u k·∫øt qu·∫£ clustering
    
    Args:
        communities_df: Community assignments
        community_sizes: Community size statistics
        output_path: Output path
    """
    print_section("L∆ØU K·∫æT QU·∫¢ CLUSTERING")
    
    # Save community assignments
    assignments_path = f"{output_path}/community_assignments"
    print(f"üíæ L∆∞u community assignments: {assignments_path}")
    
    communities_df.write \
        .mode("overwrite") \
        .parquet(assignments_path)
    
    print("‚úÖ ƒê√£ l∆∞u assignments!")
    
    # Save community sizes
    sizes_path = f"{output_path}/community_sizes"
    print(f"\nüíæ L∆∞u community sizes: {sizes_path}")
    
    community_sizes.write \
        .mode("overwrite") \
        .parquet(sizes_path)
    
    print("‚úÖ ƒê√£ l∆∞u sizes!")
    
    # Save as CSV for easy viewing
    csv_path = f"{output_path}/communities_csv"
    print(f"\nüíæ L∆∞u CSV: {csv_path}")
    
    communities_df.join(community_sizes, on="community_id") \
        .orderBy("community_id", desc("pagerank")) \
        .coalesce(1) \
        .write \
        .mode("overwrite") \
        .option("header", "true") \
        .csv(csv_path)
    
    print("‚úÖ ƒê√£ l∆∞u CSV!")


def main():
    """Main execution"""
    
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë                                                                ‚ïë
    ‚ïë      NYC TAXI GRAPH MINING - B∆Ø·ªöC 3: GRAPH CLUSTERING         ‚ïë
    ‚ïë                                                                ‚ïë
    ‚ïë  M·ª•c ti√™u: Ph√°t hi·ªán communities (clusters) c·ªßa taxi zones   ‚ïë
    ‚ïë  Ph∆∞∆°ng ph√°p: Label Propagation Algorithm                     ‚ïë
    ‚ïë                                                                ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # Create Spark session
    spark = create_spark_session("NYC_Taxi_Clustering")
    
    try:
        # Step 1: Load data
        edges_df, pagerank_df = load_graph_data(spark)
        
        # Step 2: Create graph
        graph = create_graph_with_pagerank(edges_df, pagerank_df)
        
        # Step 3: Run Label Propagation
        communities_df = run_label_propagation(graph)
        
        # Step 4: Analyze communities
        community_sizes = analyze_communities(communities_df)
        
        # Step 5: Detailed analysis
        analyze_community_details(communities_df, edges_df)
        
        # Step 6: Save results
        save_clustering_results(communities_df, community_sizes, HDFS_RESULTS)
        
        print("\n" + "="*70)
        print("üéâ HO√ÄN TH√ÄNH B∆Ø·ªöC 3: GRAPH CLUSTERING")
        print("="*70)
        print(f"\nüìÇ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {HDFS_RESULTS}")
        print("\nüìå Next steps:")
        print("   1. Ch·∫°y 4_visualization.py ƒë·ªÉ visualize k·∫øt qu·∫£")
        print("   2. Ph√¢n t√≠ch √Ω nghƒ©a c·ªßa c√°c communities")
        
    except Exception as e:
        print(f"\n‚ùå L·ªñI: {str(e)}")
        import traceback
        traceback.print_exc()
        
    finally:
        # Stop Spark
        spark.stop()
        print("\nüõë ƒê√£ d·ª´ng Spark session")


if __name__ == "__main__":
    main()
