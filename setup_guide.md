# HÆ°á»›ng dáº«n cÃ i Ä‘áº·t chi tiáº¿t - NYC Taxi Graph Mining

## PHáº¦N 1: CÃ€I Äáº¶T HADOOP + SPARK CLUSTER

### 1.1. Chuáº©n bá»‹ mÃ¡y áº£o Ubuntu

**TrÃªn cáº£ 2 mÃ¡y Ubuntu (master vÃ  worker):**

```bash
# Update há»‡ thá»‘ng
sudo apt update && sudo apt upgrade -y

# CÃ i Ä‘áº·t Java 11 (báº¯t buá»™c cho Hadoop/Spark)
sudo apt install openjdk-11-jdk -y

# Kiá»ƒm tra Java
java -version

# CÃ i Ä‘áº·t Python 3 vÃ  pip
sudo apt install python3 python3-pip python3-venv -y

# Táº¡o virtual environment (trÃªn cáº£ 2 nodes)
python3 -m venv ~/mmds-venv
source ~/mmds-venv/bin/activate

# CÃ i Ä‘áº·t dependencies
pip install --upgrade pip
pip install pyspark numpy pandas matplotlib seaborn networkx

# Deactivate venv
deactivate

# CÃ i Ä‘áº·t SSH (Ä‘á»ƒ cluster nodes giao tiáº¿p)
sudo apt install openssh-server -y
sudo systemctl enable ssh
sudo systemctl start ssh
```

---

### 1.1.1. **KHUYáº¾N NGHá»Š: ÄÃ³ng gÃ³i vÃ  phÃ¢n phá»‘i venv qua HDFS**

Äá»ƒ Ä‘áº£m báº£o mÃ´i trÆ°á»ng Python Ä‘á»“ng nháº¥t trÃªn cáº£ 2 nodes:

**TrÃªn Master Node:**

```bash
# Activate venv
source ~/mmds-venv/bin/activate

# CÃ i Ä‘áº·t táº¥t cáº£ dependencies
pip install pyspark==3.5.0 numpy pandas matplotlib seaborn networkx

# ÄÃ³ng gÃ³i venv (bá» cache Ä‘á»ƒ giáº£m size)
cd ~
tar -czf mmds-venv.tar.gz \
    --exclude='mmds-venv/__pycache__' \
    --exclude='mmds-venv/**/__pycache__' \
    --exclude='mmds-venv/lib/python3.*/site-packages/*.dist-info' \
    mmds-venv/

# Kiá»ƒm tra kÃ­ch thÆ°á»›c (nÃªn < 500MB)
ls -lh mmds-venv.tar.gz

# Upload lÃªn HDFS
hdfs dfs -mkdir -p /user/taxi/python_env/
hdfs dfs -put mmds-venv.tar.gz /user/taxi/python_env/

# Kiá»ƒm tra
hdfs dfs -ls /user/taxi/python_env/
```

**Lá»£i Ã­ch:**
- âœ… MÃ´i trÆ°á»ng Python Ä‘á»“ng nháº¥t trÃªn táº¥t cáº£ executors
- âœ… KhÃ´ng cáº§n cÃ i Ä‘áº·t thá»§ cÃ´ng trÃªn Worker
- âœ… Spark tá»± Ä‘á»™ng giáº£i nÃ©n trÃªn má»—i executor

---

### 1.1.2. **ALTERNATIVE: CÃ i thá»§ cÃ´ng trÃªn Worker (náº¿u khÃ´ng dÃ¹ng HDFS archive)**

**TrÃªn Worker Node:**

```bash
# SSH vÃ o worker
ssh worker1

# Táº¡o venv giá»‘ng Master
python3 -m venv ~/mmds-venv
source ~/mmds-venv/bin/activate

# CÃ i dependencies (pháº£i GIá»NG Master)
pip install --upgrade pip
pip install pyspark==3.5.0 numpy pandas matplotlib seaborn networkx

# Verify
python3 -c "import pyspark, numpy, pandas; print('OK')"

deactivate
```

---

### 1.1.3. Lá»‡nh spark-submit Ä‘Ãºng chuáº©n

**Option 1: Sá»­ dá»¥ng HDFS archive (KHUYáº¾N NGHá»Š)**

```bash
spark-submit \
    --master spark://master:7077 \
    --deploy-mode client \
    --driver-memory 500m \
    --executor-memory 500m \
    --executor-cores 1 \
    --num-executors 2 \
    --archives hdfs://master:9000/user/taxi/python_env/mmds-venv.tar.gz#mmds-venv \
    --conf spark.pyspark.python=./mmds-venv/bin/python3 \
    --conf spark.pyspark.driver.python=python3 \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/1_build_graph.py
```

**Giáº£i thÃ­ch:**
- `--archives`: Spark download tá»« HDFS vÃ  giáº£i nÃ©n trÃªn má»—i executor
- `#mmds-venv`: TÃªn thÆ° má»¥c sau khi giáº£i nÃ©n
- `./mmds-venv/bin/python3`: Executor dÃ¹ng Python tá»« thÆ° má»¥c nÃ y

**Option 2: Sá»­ dá»¥ng venv Ä‘Ã£ cÃ i sáºµn trÃªn Worker**

```bash
spark-submit \
    --master spark://master:7077 \
    --deploy-mode client \
    --driver-memory 500m \
    --executor-memory 500m \
    --executor-cores 1 \
    --num-executors 2 \
    --conf spark.pyspark.python=/home/tiennd/mmds-venv/bin/python3 \
    --conf spark.pyspark.driver.python=python3 \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/1_build_graph.py
```

**ChÃº Ã½:** ÄÆ°á»ng dáº«n `/home/tiennd/mmds-venv/bin/python3` pháº£i tá»“n táº¡i trÃªn **Táº¤T Cáº¢** nodes.

---

### 1.1.4. Kiá»ƒm tra mÃ´i trÆ°á»ng Python trÃªn Worker

**Test trÃªn Worker:**

```bash
# SSH vÃ o worker
ssh worker1

# Activate venv
source ~/mmds-venv/bin/activate

# Test imports
python3 << EOF
import pyspark
import numpy
import pandas
print("PySpark version:", pyspark.__version__)
print("NumPy version:", numpy.__version__)
print("Pandas version:", pandas.__version__)
print("âœ… All imports OK!")
EOF

deactivate
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
PySpark version: 3.5.0
NumPy version: 1.26.x
Pandas version: 2.x.x
âœ… All imports OK!
```

---

### 1.2. Thiáº¿t láº­p SSH passwordless giá»¯a cÃ¡c nodes

**TrÃªn Master Node:**

```bash
# Táº¡o SSH key
ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa

# Copy public key sang chÃ­nh nÃ³
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# Copy sang Worker Node (thay <worker-ip> báº±ng IP thá»±c táº¿)
ssh-copy-id user@<worker-ip>

# Test káº¿t ná»‘i
ssh user@<worker-ip>
```

**LÆ°u Ã½ IP addresses:**
- Master: 192.168.x.x (vÃ­ dá»¥: 192.168.56.101)
- Worker: 192.168.x.x (vÃ­ dá»¥: 192.168.56.102)

```bash
# Kiá»ƒm tra IP
ip addr show
```

---
window
Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0


### 1.3. CÃ i Ä‘áº·t Hadoop (Pseudo-Distributed hoáº·c Fully-Distributed)

> **âš ï¸ QUAN TRá»ŒNG: CÃ¡c bÆ°á»›c sau cháº¡y trÃªn CÃ 2 NODES (Master vÃ  Worker)**
> 
> Tuy nhiÃªn, má»™t sá»‘ bÆ°á»›c chá»‰ cháº¡y trÃªn Master. Sáº½ Ä‘Æ°á»£c chÃº thÃ­ch rÃµ rÃ ng.

---

**Táº£i Hadoop (trÃªn Cáº¢ 2 NODES - Master vÃ  Worker):**

```bash
cd ~
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar -xzvf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /opt/hadoop
```

---

**Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng (trÃªn Cáº¢ 2 NODES):**

ThÃªm vÃ o `~/.bashrc` trÃªn cáº£ Master vÃ  Worker:

```bash
echo "export HADOOP_HOME=/opt/hadoop" >> ~/.bashrc
echo "export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop" >> ~/.bashrc
echo "export PATH=\$PATH:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin" >> ~/.bashrc
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> ~/.bashrc
    or echo "export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64" >> ~/.bashrc
source ~/.bashrc
```

---

**Cáº¥u hÃ¬nh Hadoop (trÃªn Cáº¢ 2 NODES):**

> **LÆ°u Ã½:** Cáº¥u hÃ¬nh pháº£i GIá»NG NHAU trÃªn cáº£ Master vÃ  Worker Ä‘á»ƒ cluster hoáº¡t Ä‘á»™ng Ä‘á»“ng bá»™.

**File core-site.xml (trÃªn Cáº¢ 2 NODES):**

```bash
nano $HADOOP_HOME/etc/hadoop/core-site.xml
```

ThÃªm ná»™i dung:

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/tiennd/hadoop_tmp</value>
    </property>
</configuration>
```

---

**File hdfs-site.xml (trÃªn Cáº¢ 2 NODES):**

```bash
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/tiennd/hadoop_data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/tiennd/hadoop_data/datanode</value>
    </property>
</configuration>
```

---

**File yarn-site.xml (trÃªn Cáº¢ 2 NODES):**

```bash
nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>2</value>
    </property>
</configuration>
```

---

**File mapred-site.xml (trÃªn Cáº¢ 2 NODES):**

```bash
nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>
```

---

**Cáº¥u hÃ¬nh workers (trÃªn Cáº¢ 2 NODES):**

```bash
nano $HADOOP_HOME/etc/hadoop/workers
```

Ná»™i dung (giá»‘ng nhau trÃªn cáº£ 2 nodes, xÃ³a má»¥c localhost náº¿u cÃ³):

```
master
worker1
```

---

**Thiáº¿t láº­p /etc/hosts (trÃªn Cáº¢ 2 NODES):**

```bash
sudo nano /etc/hosts
```

ThÃªm (thay IP thá»±c táº¿ cá»§a báº¡n):

```
192.168.56.101  master
192.168.56.102  worker1
```

---

**Táº¡o thÆ° má»¥c vÃ  format HDFS:**

> **âš ï¸ CHÃš Ã: Pháº§n nÃ y CHá»ˆ QUAN TRá»ŒNG cho phÃ¢n biá»‡t Master vs Worker**

**TrÃªn Cáº¢ 2 NODES:** Táº¡o thÆ° má»¥c

```bash
mkdir -p ~/hadoop_tmp
mkdir -p ~/hadoop_data/namenode
mkdir -p ~/hadoop_data/datanode
```

**CHá»ˆ TRÃŠN MASTER NODE:** Format NameNode

```bash
# âš ï¸ CHá»ˆ CHáº Y Lá»†NH NÃ€Y TRÃŠN MASTER
# KHÃ”NG cháº¡y trÃªn Worker
hdfs namenode -format
```

> **Giáº£i thÃ­ch:**
> - **Master:** Cháº¡y NameNode (quáº£n lÃ½ metadata) â†’ cáº§n format
> - **Worker:** Chá»‰ cháº¡y DataNode (lÆ°u dá»¯ liá»‡u) â†’ KHÃ”NG cáº§n format

---

**Khá»Ÿi Ä‘á»™ng Hadoop:**

> **âš ï¸ Táº¤T Cáº¢ Lá»†NH KHá»I Äá»˜NG Äá»€U CHáº Y TRÃŠN MASTER NODE**
> 
> SSH passwordless sáº½ tá»± Ä‘á»™ng kÃ­ch hoáº¡t services trÃªn Worker

**CHá»ˆ TRÃŠN MASTER NODE:**

```bash
# Má»Ÿ file cáº¥u hÃ¬nh
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
# TÃ¬m dÃ²ng:
# export JAVA_HOME=
# Bá» comment vÃ  sá»­a thÃ nh:
export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64
# Copy tá»« Master sang Worker
scp $HADOOP_HOME/etc/hadoop/hadoop-env.sh tiennd@worker1:$HADOOP_HOME/etc/hadoop/

# Khá»Ÿi Ä‘á»™ng HDFS (NameNode trÃªn master, DataNode trÃªn cáº£ 2 nodes)
start-dfs.sh

# Khá»Ÿi Ä‘á»™ng YARN (ResourceManager trÃªn master, NodeManager trÃªn cáº£ 2 nodes)
start-yarn.sh


# Má»Ÿ file
nano ~/.bashrc

# ThÃªm (chá»n 1 trong 2 dÃ²ng phÃ¹ há»£p):
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
# hoáº·c
export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64

# ThÃªm bin vÃ o PATH
export PATH=$JAVA_HOME/bin:$PATH

# LÆ°u vÃ  Ã¡p dá»¥ng
source ~/.bashrc

sudo apt install openjdk-21-jdk openjdk-21-jdk-headless -y

# LÃ m cáº£ 2 node master vÃ  worker1
# Má»Ÿ file cáº¥u hÃ¬nh mÃ´i trÆ°á»ng cá»§a Hadoop: 
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# TÃ¬m Ä‘áº¿n pháº§n cáº¥u hÃ¬nh HADOOP_OPTS hoáº·c thÃªm vÃ o cuá»‘i file dÃ²ng sau:
export HADOOP_OPTS="$HADOOP_OPTS --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.util=ALL-UNNAMED --add-opens java.base/java.lang.reflect=ALL-UNNAMED --add-opens java.base/java.text=ALL-UNNAMED --add-opens java.desktop/java.awt.font=ALL-UNNAMED"

# Kiá»ƒm tra trÃªn MASTER
jps  
# NÃªn tháº¥y: NameNode, DataNode, SecondaryNameNode, ResourceManager, NodeManager

# Kiá»ƒm tra trÃªn WORKER (SSH vÃ o Worker Ä‘á»ƒ check)
ssh worker1 "jps"
# NÃªn tháº¥y: DataNode, NodeManager
```

**Xem Web UI:**

```bash
# HDFS NameNode: http://master:9870
# YARN ResourceManager: http://master:8088
```

---

**ğŸ“ TÃ“M Táº®T - AI LÃ€M GÃŒ:**

| BÆ°á»›c | Master Node | Worker Node |
|------|-------------|-------------|
| Download Hadoop | âœ… CÃ³ | âœ… CÃ³ |
| Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng | âœ… CÃ³ | âœ… CÃ³ |
| Cáº¥u hÃ¬nh XML files | âœ… CÃ³ (giá»‘ng nhau) | âœ… CÃ³ (giá»‘ng nhau) |
| Edit /etc/hosts | âœ… CÃ³ | âœ… CÃ³ |
| Táº¡o thÆ° má»¥c | âœ… CÃ³ | âœ… CÃ³ |
| **Format NameNode** | âœ… **CHá»ˆ Master** | âŒ **KHÃ”NG** |
| **Khá»Ÿi Ä‘á»™ng services** | âœ… **CHá»ˆ Master** (SSH tá»± Ä‘á»™ng start Worker) | âŒ Tá»± Ä‘á»™ng bá»Ÿi Master |
| Kiá»ƒm tra `jps` | âœ… NameNode + DataNode + RM + NM | âœ… DataNode + NodeManager |

---

**ğŸ’¡ Máº¸O COPY Cáº¤U HÃŒNH:**

Náº¿u khÃ´ng muá»‘n config thá»§ cÃ´ng trÃªn Worker, cÃ³ thá»ƒ copy tá»« Master:

```bash
# TrÃªn Master, sau khi cáº¥u hÃ¬nh xong
scp -r /opt/hadoop/etc/hadoop/* user@worker1:/opt/hadoop/etc/hadoop/

# Copy .bashrc
scp ~/.bashrc user@worker1:~/
ssh worker1 "source ~/.bashrc"
```

NhÆ°ng **váº«n pháº£i** táº¡o thÆ° má»¥c vÃ  edit /etc/hosts trÃªn Worker.

### 1.4. CÃ i Ä‘áº·t Apache Spark

> **âš ï¸ QUAN TRá»ŒNG: CÃ¡c bÆ°á»›c sau cháº¡y trÃªn Cáº¢ 2 NODES (Master vÃ  Worker)**
> 
> TÆ°Æ¡ng tá»± Hadoop, Spark cáº§n Ä‘Æ°á»£c cÃ i Ä‘áº·t trÃªn cáº£ Master vÃ  Worker Ä‘á»ƒ cluster hoáº¡t Ä‘á»™ng.

---

**Táº£i Spark (trÃªn Cáº¢ 2 NODES - Master vÃ  Worker):**

```bash
cd ~
wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
tar -xzvf spark-3.5.0-bin-hadoop3.tgz
sudo mv spark-3.5.0-bin-hadoop3 /opt/spark
```

---

**Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng (trÃªn Cáº¢ 2 NODES):**

```bash
echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
echo "export PATH=\$PATH:\$SPARK_HOME/bin:\$SPARK_HOME/sbin" >> ~/.bashrc
echo "export PYSPARK_PYTHON=python3" >> ~/.bashrc
source ~/.bashrc
```

---

**Cáº¥u hÃ¬nh Spark (trÃªn Cáº¢ 2 NODES):**

> **LÆ°u Ã½:** Cáº¥u hÃ¬nh pháº£i GIá»NG NHAU trÃªn cáº£ Master vÃ  Worker.

```bash
cd $SPARK_HOME/conf
cp spark-env.sh.template spark-env.sh
nano spark-env.sh
```

ThÃªm (trÃªn cáº£ 2 nodes):

```bash
export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
export SPARK_MASTER_HOST=master
export SPARK_WORKER_CORES=2
export SPARK_WORKER_MEMORY=2g
export SPARK_DRIVER_MEMORY=1g
```

---

**Cáº¥u hÃ¬nh workers (trÃªn Cáº¢ 2 NODES):**

```bash
cp workers.template workers
nano workers
```

Ná»™i dung (giá»‘ng nhau trÃªn cáº£ 2 nodes):

```
master
worker1
```

---

**Khá»Ÿi Ä‘á»™ng Spark Standalone Cluster:**
> **âš ï¸ Táº¤T Cáº¢ Lá»†NH KHá»I Äá»˜NG Äá»€U CHáº Y TRÃŠN MASTER NODE**
> 
> SSH passwordless sáº½ tá»± Ä‘á»™ng kÃ­ch hoáº¡t Worker nodes

**CHá»ˆ TRÃŠN MASTER NODE:**

```bash
# Khá»Ÿi Ä‘á»™ng Spark Master
start-master.sh

# Khá»Ÿi Ä‘á»™ng Spark Workers (tá»± Ä‘á»™ng trÃªn cáº£ Master vÃ  Worker)
start-workers.sh

# Kiá»ƒm tra trÃªn MASTER
jps  
# NÃªn tháº¥y: Master, Worker (náº¿u Master cÅ©ng lÃ  Worker)

# Kiá»ƒm tra trÃªn WORKER
ssh worker1 "jps"
# NÃªn tháº¥y: Worker

# Web UI: http://master:8080
```

---

**ğŸ’¡ Máº¸O COPY Cáº¤U HÃŒNH:**

Náº¿u Ä‘Ã£ cáº¥u hÃ¬nh xong trÃªn Master, cÃ³ thá»ƒ copy sang Worker:

```bash
# TrÃªn Master
scp -r /opt/spark/conf/* user@worker1:/opt/spark/conf/

# Copy .bashrc (pháº§n Spark)
scp ~/.bashrc user@worker1:~/
ssh worker1 "source ~/.bashrc"
```

---

**ğŸ“ TÃ“M Táº®T - AI LÃ€M GÃŒ:**

| BÆ°á»›c | Master Node | Worker Node |
|------|-------------|-------------|
| Download Spark | âœ… CÃ³ | âœ… CÃ³ |
| Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng | âœ… CÃ³ | âœ… CÃ³ |
| Cáº¥u hÃ¬nh spark-env.sh | âœ… CÃ³ (giá»‘ng nhau) | âœ… CÃ³ (giá»‘ng nhau) |
| Cáº¥u hÃ¬nh workers file | âœ… CÃ³ (giá»‘ng nhau) | âœ… CÃ³ (giá»‘ng nhau) |
| **Khá»Ÿi Ä‘á»™ng services** | âœ… **CHá»ˆ Master** (SSH tá»± Ä‘á»™ng start Worker) | âŒ Tá»± Ä‘á»™ng bá»Ÿi Master |
| Kiá»ƒm tra `jps` | âœ… Master + Worker | âœ… Worker |

### 1.5. CÃ i Ä‘áº·t GraphFrames

> **âš ï¸ CÃ€I Äáº¶T TRÃŠN Cáº¢ 2 NODES (Master vÃ  Worker)**

```bash
# Táº£i GraphFrames
cd ~
wget https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.12/graphframes-0.8.3-spark3.5-s_2.12.jar
sudo cp graphframes-0.8.3-spark3.5-s_2.12.jar $SPARK_HOME/jars/

# Hoáº·c cÃ i qua pip
pip3 install graphframes
```

---

## PHáº¦N 2: DOWNLOAD VÃ€ CHUáº¨N Bá»Š Dá»® LIá»†U

### 2.1. Download NYC TLC Yellow Taxi Data

> **âš ï¸ CHá»ˆ CHáº Y TRÃŠN MASTER NODE**
> 
> Chá»‰ cáº§n download dá»¯ liá»‡u trÃªn Master, sau Ä‘Ã³ upload lÃªn HDFS. HDFS sáº½ tá»± Ä‘á»™ng phÃ¢n tÃ¡n dá»¯ liá»‡u sang cÃ¡c Worker nodes.

**Script download dá»¯ liá»‡u (1-2 nÄƒm Ä‘á»ƒ Ä‘á»§ massive):**

**CHá»ˆ TRÃŠN MASTER NODE:**
```bash
mkdir -p ~/nyc_taxi_data
cd ~/nyc_taxi_data

# Download tá»« thÃ¡ng 1/2019 Ä‘áº¿n 12/2020 (24 thÃ¡ng ~ 30GB)
for year in 2019 2020; do
    for month in {01..12}; do
        wget "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_${year}-${month}.parquet"
    done
done
```
```bash
mkdir -p ~/nyc_taxi_data
cd ~/nyc_taxi_data

for year in 2025; do
    for month in {01..12}; do
        wget "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_${year}-${month}.parquet"
    done
done
```

> **ğŸ’¡ Giáº£i thÃ­ch:**
> - **Táº¡i sao chá»‰ trÃªn Master?** HDFS sáº½ tá»± Ä‘á»™ng replicate dá»¯ liá»‡u sang Worker khi upload
> - **Tiáº¿t kiá»‡m bÄƒng thÃ´ng:** KhÃ´ng cáº§n download trÃ¹ng láº·p trÃªn nhiá»u mÃ¡y
> - **Quáº£n lÃ½ táº­p trung:** Dá»… kiá»ƒm soÃ¡t version vÃ  tÃ­nh toÃ n váº¹n dá»¯ liá»‡u

**Náº¿u link khÃ´ng hoáº¡t Ä‘á»™ng, dÃ¹ng alternative:**

```bash
# Táº£i tá»« NYC TLC website
# https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
```

### 2.2. Upload dá»¯ liá»‡u lÃªn HDFS

> **âš ï¸ CHá»ˆ CHáº Y TRÃŠN MASTER NODE**

**CHá»ˆ TRÃŠN MASTER NODE:**
```bash
# Táº¡o thÆ° má»¥c trÃªn HDFS
# âš ï¸ LÆ¯U Ã: /user/ lÃ  thÆ° má»¥c convention cá»§a HDFS, KHÃ”NG pháº£i user OS cá»§a báº¡n
# Báº¡n cÃ³ thá»ƒ Ä‘áº·t tÃªn báº¥t ká»³, vÃ­ dá»¥:
hdfs dfs -mkdir -p /user/taxi/raw_data
# hoáº·c
hdfs dfs -mkdir -p /user/tiennd/taxi/raw_data
# hoáº·c
hdfs dfs -mkdir -p /data/taxi/raw_data

# Upload dá»¯ liá»‡u (Ä‘iá»u chá»‰nh path tÆ°Æ¡ng á»©ng)
hdfs dfs -put ~/nyc_taxi_data/*.parquet /user/taxi/raw_data/

# Kiá»ƒm tra
hdfs dfs -ls /user/taxi/raw_data/
hdfs dfs -df -h  # Xem dung lÆ°á»£ng

# Kiá»ƒm tra replication (dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c sao sang Worker chÆ°a)
hdfs fsck /user/taxi/raw_data/ -files -blocks -locations
```

> **ğŸ’¡ Giáº£i thÃ­ch vá» Ä‘Æ°á»ng dáº«n HDFS:**
> - `/user/` lÃ  **CONVENTION** cá»§a HDFS (giá»‘ng `/home/` trong Linux), KHÃ”NG liÃªn quan Ä‘áº¿n user OS
> - Báº¡n hoÃ n toÃ n tá»± do Ä‘áº·t: `/data/`, `/project/`, `/taxi_mining/`, etc.
> - Náº¿u muá»‘n theo username: `/user/tiennd/taxi/` (nhÆ°ng khÃ´ng báº¯t buá»™c)
> - **Quan trá»ng:** Nháº¥t quÃ¡n Ä‘Æ°á»ng dáº«n trong toÃ n bá»™ project

> **ğŸ’¡ Sau khi upload:**
> - HDFS tá»± Ä‘á»™ng replicate dá»¯ liá»‡u sang Worker theo cáº¥u hÃ¬nh `dfs.replication=2`
> - CÃ³ thá»ƒ xem phÃ¢n bá»‘ block trÃªn Web UI: http://master:9870

### 2.3. Download Taxi Zone Lookup

> **âš ï¸ CHá»ˆ CHáº Y TRÃŠN MASTER NODE**

**CHá»ˆ TRÃŠN MASTER NODE:**
```bash
cd ~/nyc_taxi_data
wget "https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"

# Upload vÃ o cÃ¹ng namespace vá»›i dá»¯ liá»‡u chÃ­nh
hdfs dfs -put taxi+_zone_lookup.csv /user/taxi/
# hoáº·c náº¿u dÃ¹ng path khÃ¡c:
# hdfs dfs -put taxi+_zone_lookup.csv /user/tiennd/taxi/
```

---

**ğŸ“ Máº¸O QUáº¢N LÃ HDFS PATH:**

```bash
# Option 1: Theo convention /user/<username>/ (giá»‘ng Linux)
hdfs dfs -mkdir -p /user/tiennd/taxi/{raw_data,processed,results}
hdfs dfs -put *.parquet /user/tiennd/taxi/raw_data/

# Option 2: Theo tÃªn project
hdfs dfs -mkdir -p /taxi_mining/{raw,processed,graphs,results}
hdfs dfs -put *.parquet /taxi_mining/raw/

# Option 3: ÄÆ¡n giáº£n nháº¥t
hdfs dfs -mkdir -p /taxi/raw_data
hdfs dfs -put *.parquet /taxi/raw_data/

# Xem toÃ n bá»™ HDFS
hdfs dfs -ls /
hdfs dfs -ls -R /user/
```

**ğŸ“Œ KHUYáº¾N NGHá»Š:**
- Chá»n 1 cáº¥u trÃºc vÃ  giá»¯ nháº¥t quÃ¡n
- Äá» xuáº¥t: `/user/tiennd/taxi/` (dá»… phÃ¢n quyá»n vÃ  quáº£n lÃ½ sau nÃ y)

---

**ğŸ“ TÃ“M Táº®T - PHáº¦N 2:**

| BÆ°á»›c | Master Node | Worker Node |
|------|-------------|-------------|
| Download dá»¯ liá»‡u thÃ´ | âœ… **CHá»ˆ Master** | âŒ KhÃ´ng cáº§n |
| Upload lÃªn HDFS | âœ… **CHá»ˆ Master** | âŒ Tá»± Ä‘á»™ng nháº­n tá»« HDFS |
| LÆ°u trá»¯ HDFS blocks | âœ… CÃ³ (NameNode + DataNode) | âœ… CÃ³ (DataNode - tá»± Ä‘á»™ng replicate) |
| Äá»c dá»¯ liá»‡u tá»« HDFS | âœ… CÃ³ | âœ… CÃ³ |

---

## PHáº¦N 3: IMPLEMENTATION CODE

> **âš ï¸ QUAN TRá»ŒNG: CODE CHáº Y á» ÄÃ‚U?**
> 
> - **Development & Submit Jobs:** Cháº¡y trÃªn **MASTER NODE** (hoáº·c mÃ¡y client báº¥t ká»³ cÃ³ káº¿t ná»‘i cluster)
> - **Execution:** Spark tá»± Ä‘á»™ng phÃ¢n tÃ¡n tasks sang **Cáº¢ 2 NODES** (Master + Worker)
> - **Notebooks:** Cháº¡y trÃªn **MASTER NODE** (hoáº·c laptop cá»§a báº¡n náº¿u cÃ³ káº¿t ná»‘i)

### 3.1. Cáº¥u trÃºc project

> **ğŸ“Œ Khuyáº¿n nghá»‹:** Táº¡o project trÃªn **MASTER NODE** táº¡i `~/massive_data_mining/`

```bash
# TrÃªn MASTER NODE
cd ~
mkdir -p massive_data_mining/{data,notebooks,src,config,results,docs}
cd massive_data_mining
```
cd ~
mkdir -p massive_data_mining
cd massive_data_mining

**Cáº¥u trÃºc thÆ° má»¥c:**
```
~/massive_data_mining/              # âš ï¸ Táº¡o trÃªn MASTER NODE
â”œâ”€â”€ data/                           # Dá»¯ liá»‡u local (sample nhá» Ä‘á»ƒ test)
â”œâ”€â”€ notebooks/                      # Jupyter notebooks (cháº¡y trÃªn Master)
â”‚   â”œâ”€â”€ 1_explore_data.ipynb
â”‚   â”œâ”€â”€ 2_build_graph.ipynb
â”‚   â”œâ”€â”€ 3_pagerank_analysis.ipynb
â”‚   â””â”€â”€ 4_clustering.ipynb
â”œâ”€â”€ src/                            # Python scripts
â”‚   â”œâ”€â”€ 1_build_graph.py           # Build edge list tá»« trip data
â”‚   â”œâ”€â”€ 2_pagerank.py              # PageRank implementation
â”‚   â”œâ”€â”€ 3_clustering.py            # Graph clustering (Label Propagation, etc.)
â”‚   â”œâ”€â”€ 4_visualization.py         # Váº½ Ä‘á»“ thá»‹ vÃ  charts
â”‚   â””â”€â”€ utils.py                   # Helper functions
â”œâ”€â”€ config/
â”‚   â””â”€â”€ spark_config.py            # Spark configuration
â”œâ”€â”€ results/                        # Káº¿t quáº£ output (local trÃªn Master)
â”‚   â”œâ”€â”€ graphs/
â”‚   â”œâ”€â”€ pagerank/
â”‚   â””â”€â”€ clusters/
â”œâ”€â”€ docs/                           # BÃ¡o cÃ¡o
â”‚   â””â”€â”€ report.md
â””â”€â”€ README.md
```

---

### 3.2. Workflow thá»±c táº¿

> **ğŸ’¡ Hiá»ƒu rÃµ luá»“ng xá»­ lÃ½:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MASTER NODE (~/massive_data_mining/)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Viáº¿t code Python/Notebook                        â”‚  â”‚
â”‚  â”‚  2. Submit job: spark-submit script.py               â”‚  â”‚
â”‚  â”‚     --master spark://master:7077                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Spark Driver (trÃªn Master)                          â”‚  â”‚
â”‚  â”‚  - Äá»c dá»¯ liá»‡u tá»« HDFS                               â”‚  â”‚
â”‚  â”‚  - Táº¡o execution plan                                â”‚  â”‚
â”‚  â”‚  - PhÃ¢n phá»‘i tasks                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                    â”‚
        â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MASTER NODE       â”‚            â”‚  WORKER NODE       â”‚
â”‚  Spark Worker      â”‚            â”‚  Spark Worker      â”‚
â”‚  - Execute tasks   â”‚            â”‚  - Execute tasks   â”‚
â”‚  - Read HDFS data  â”‚            â”‚  - Read HDFS data  â”‚
â”‚  - Process         â”‚            â”‚  - Process         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Results               â”‚
              â”‚  - HDFS: /results/     â”‚
              â”‚  - Local: ~/results/   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3.3. CÃ¡ch cháº¡y code

**Option 1: Cháº¡y trá»±c tiáº¿p trÃªn Master (Development)**

```bash
# SSH vÃ o Master
ssh tiennd@master

# Navigate to project
cd ~/massive_data_mining

# comment line graphframes the run, may need creat venv:
pip install -r requirement.txt

# ÄÃ³ng gÃ³i venv (bá» cache Ä‘á»ƒ giáº£m size)
tar -czf mmds-venv.tar.gz \
    --exclude='mmds-venv/__pycache__' \
    --exclude='mmds-venv/**/__pycache__' \
    --exclude='mmds-venv/lib/python3.*/site-packages/*.dist-info' \
    mmds-venv/
# Kiá»ƒm tra kÃ­ch thÆ°á»›c
ls -lh mmds-venv.tar.gz

# Táº¡o thÆ° má»¥c trÃªn HDFS
hdfs dfs -mkdir -p /user/taxi/python_env/

# Upload venv
hdfs dfs -put mmds-venv.tar.gz /user/taxi/python_env/

# Kiá»ƒm tra
hdfs dfs -ls /user/taxi/python_env/

# Cháº¡y script Python
python3 src/1_build_graph.py

# Hoáº·c submit Spark job
spark-submit \
    --master spark://master:7077 \
    --deploy-mode client \
    --driver-memory 500m \
    --executor-memory 500m \
    --executor-cores 1 \
    --num-executors 2 \
    --archives hdfs://master:9000/user/taxi/python_env/mmds-venv.tar.gz#mmds-venv \
    --conf spark.pyspark.python=./mmds-venv/bin/python3 \
    --conf spark.pyspark.driver.python=python3 \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/1_build_graph.py
```

**Option 2: Jupyter Notebook trÃªn Master**

```bash
# TrÃªn Master, cÃ i Jupyter
pip3 install jupyter

# Khá»Ÿi Ä‘á»™ng Jupyter (cho phÃ©p remote access)
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser

# Tá»« laptop cá»§a báº¡n, má»Ÿ browser:
# http://master:8888
# Hoáº·c SSH tunnel:
ssh -L 8888:localhost:8888 tiennd@master
# Má»Ÿ: http://localhost:8888
```

**Option 3: Remote Development tá»« Windows (VSCode)**

```bash
# TrÃªn Windows, dÃ¹ng VSCode Remote SSH
# 1. Install extension: "Remote - SSH"
# 2. Connect to Master: ssh tiennd@master
# 3. Má»Ÿ folder: ~/massive_data_mining
# 4. Code vÃ  debug trá»±c tiáº¿p trÃªn Master
```

---

### 3.4. LÆ°u Ã½ quan trá»ng

**ğŸ“Œ Files cáº§n á»Ÿ Ä‘Ã¢u:**
| File/Folder | Location | LÃ½ do |
|-------------|----------|-------|
| **Code Python (.py)** | Master: `~/massive_data_mining/src/` | Submit tá»« Master |
| **Notebooks (.ipynb)** | Master: `~/massive_data_mining/notebooks/` | Jupyter cháº¡y trÃªn Master |
| **Spark config** | Master: `~/massive_data_mining/config/` | Driver Ä‘á»c config |
| **Input data (raw)** | HDFS: `/user/taxi/raw_data/` | Táº¥t cáº£ nodes Ä‘á»u Ä‘á»c tá»« HDFS |
| **Output results** | HDFS: `/user/taxi/results/` HOáº¶C Local: `~/results/` | HDFS cho big data, Local cho reports |
| **Visualizations** | Local Master: `~/massive_data_mining/results/` | Download vá» Ä‘á»ƒ xem |

**ğŸ’¡ Best Practices:**
1. **Code trÃªn Master, cháº¡y distributed:**
   ```bash
   # Code á»Ÿ: ~/massive_data_mining/src/pagerank.py
   # Data á»Ÿ: hdfs://master:9000/user/taxi/raw_data/
   # Submit: spark-submit --master spark://master:7077 src/pagerank.py
   ```

2. **KhÃ´ng cáº§n copy code sang Worker:**
   - Spark tá»± Ä‘á»™ng serialize vÃ  gá»­i code tá»›i Worker
   - Chá»‰ cáº§n Ä‘áº£m báº£o Worker cÃ³ cÃ i dependencies (pyspark, networkx, etc.)

3. **Káº¿t quáº£ nhá» â†’ Local, káº¿t quáº£ lá»›n â†’ HDFS:**
   ```python
   # Káº¿t quáº£ nhá» (vÃ i MB)
   pagerank_df.toPandas().to_csv("~/results/pagerank.csv")
   
   # Káº¿t quáº£ lá»›n (vÃ i GB)
   pagerank_df.write.parquet("hdfs://master:9000/user/taxi/results/pagerank/")
   ```

---

**ğŸ“ TÃ“M Táº®T - AI LÃ€M GÃŒ:**

| Task | Master Node | Worker Node |
|------|-------------|-------------|
| Viáº¿t code | âœ… CÃ³ | âŒ KhÃ´ng |
| Submit Spark jobs | âœ… CÃ³ | âŒ KhÃ´ng |
| Cháº¡y Jupyter | âœ… CÃ³ | âŒ KhÃ´ng |
| Execute Spark tasks | âœ… CÃ³ (Worker role) | âœ… CÃ³ |
| Äá»c dá»¯ liá»‡u tá»« HDFS | âœ… CÃ³ | âœ… CÃ³ |
| LÆ°u results local | âœ… CÃ³ | âŒ KhÃ´ng cáº§n |
| LÆ°u results HDFS | âœ… Driver ghi | âœ… Executors ghi (distributed) |

---

## PHáº¦N 4: KIá»‚M TRA Há»† THá»NG

### 4.1. Test Hadoop

```bash
# Test HDFS
hdfs dfs -mkdir /test
hdfs dfs -put /etc/hosts /test/
hdfs dfs -cat /test/hosts
hdfs dfs -rm -r /test

# Test MapReduce
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi 2 100
```

### 4.2. Test Spark

```bash
# Test Spark standalone
spark-submit --class org.apache.spark.examples.SparkPi \
    --master spark://master:7077 \
    $SPARK_HOME/examples/jars/spark-examples_*.jar 100

# Test PySpark
pyspark --master spark://master:7077
```

Trong PySpark shell:

```python
# Test RDD
rdd = sc.parallelize(range(1000))
print(rdd.sum())

# Test DataFrame
df = spark.createDataFrame([(1, "a"), (2, "b")], ["id", "value"])
df.show()
```

### 4.3. Test Ä‘á»c Parquet tá»« HDFS

```python
# pyspark --master spark://master:7077

df = spark.read.parquet("hdfs://master:9000/user/taxi/raw_data/yellow_tripdata_2019-01.parquet")
print(f"Sá»‘ dÃ²ng: {df.count()}")
df.printSchema()
df.show(5)
```

---

## LÆ¯U Ã QUAN TRá»ŒNG

### Memory Management

Vá»›i RAM háº¡n cháº¿, cáº§n tune cáº©n tháº­n:

```bash
# spark-defaults.conf
spark.driver.memory              2g
spark.executor.memory            2g
spark.executor.cores             2
spark.default.parallelism        8
spark.sql.shuffle.partitions     200
spark.memory.fraction            0.8
spark.memory.storageFraction     0.3
```

### Monitoring

- HDFS UI: http://master:9870
- YARN UI: http://master:8088  
- Spark UI: http://master:8080
- Spark Application UI: http://master:4040 (khi job cháº¡y)

### Troubleshooting

```bash
# Xem logs
tail -f $HADOOP_HOME/logs/*
tail -f $SPARK_HOME/logs/*

# Restart services náº¿u cÃ³ lá»—i
stop-all.sh
start-dfs.sh
start-yarn.sh
start-master.sh
start-workers.sh
```

---

**Tiáº¿p theo: Táº¡o cÃ¡c file code implementation**
