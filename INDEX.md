# ğŸ“š TÃ€I LIá»†U Tá»”NG Há»¢P Dá»° ÃN - NYC TAXI GRAPH MINING

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c hoÃ n chá»‰nh

```
massive_data_mining/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                      â† Tá»•ng quan dá»± Ã¡n
â”œâ”€â”€ ğŸ“„ QUICKSTART.md                  â† HÆ°á»›ng dáº«n nhanh
â”œâ”€â”€ ğŸ“„ setup_guide.md                 â† HÆ°á»›ng dáº«n cÃ i Ä‘áº·t chi tiáº¿t
â”œâ”€â”€ ğŸ“„ PRESENTATION_GUIDE.md          â† HÆ°á»›ng dáº«n bÃ¡o cÃ¡o & báº£o vá»‡
â”œâ”€â”€ ğŸ“„ PROJECT_CHECKLIST.md           â† Checklist tá»«ng bÆ°á»›c
â”œâ”€â”€ ğŸ“„ intruction.md                  â† Äá» bÃ i gá»‘c
â”œâ”€â”€ ğŸ“„ requirements.txt               â† Python dependencies
â”œâ”€â”€ ğŸ”§ run_all.sh                     â† Script cháº¡y toÃ n bá»™ pipeline
â”œâ”€â”€ ğŸ”§ check_setup.sh                 â† Script kiá»ƒm tra setup
â”‚
â”œâ”€â”€ ğŸ“‚ config/
â”‚   â””â”€â”€ spark_config.py               â† Cáº¥u hÃ¬nh Spark session
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ utils.py                      â† Utility functions
â”‚   â”œâ”€â”€ 1_build_graph.py              â† XÃ¢y dá»±ng Ä‘á»“ thá»‹ (MapReduce)
â”‚   â”œâ”€â”€ 2_pagerank.py                 â† TÃ­nh PageRank
â”‚   â”œâ”€â”€ 3_clustering.py               â† Graph clustering
â”‚   â”œâ”€â”€ 4_visualization.py            â† Táº¡o visualizations
â”‚   â””â”€â”€ 5_benchmark.py                â† Benchmark & scalability
â”‚
â”œâ”€â”€ ğŸ“‚ results/                        â† Káº¿t quáº£ (generated)
â”‚   â”œâ”€â”€ visualizations/               â† Charts, graphs
â”‚   â”‚   â”œâ”€â”€ pagerank_distribution.png
â”‚   â”‚   â”œâ”€â”€ community_analysis.png
â”‚   â”‚   â”œâ”€â”€ summary_statistics.csv
â”‚   â”‚   â””â”€â”€ top50_zones_detailed.csv
â”‚   â””â”€â”€ benchmarks/                   â† Benchmark results
â”‚       â””â”€â”€ benchmark_*.json
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                      â† Jupyter notebooks (optional)
â”‚   â”œâ”€â”€ exploration.ipynb
â”‚   â””â”€â”€ analysis.ipynb
â”‚
â””â”€â”€ ğŸ“‚ docs/                           â† TÃ i liá»‡u bÃ¡o cÃ¡o (táº¡o riÃªng)
    â”œâ”€â”€ report.pdf
    â”œâ”€â”€ slides.pptx
    â””â”€â”€ screenshots/
```

---

## ğŸ—ºï¸ Lá»˜ TRÃŒNH THá»°C HIá»†N (TIMELINE)

### Tuáº§n 1-2: Setup cÆ¡ sá»Ÿ háº¡ táº§ng
- âœ… CÃ i Ä‘áº·t VMware VMs
- âœ… CÃ i Ä‘áº·t Hadoop cluster
- âœ… CÃ i Ä‘áº·t Spark cluster
- âœ… Test káº¿t ná»‘i vÃ  services

### Tuáº§n 3: Download vÃ  upload dá»¯ liá»‡u
- âœ… Download NYC Taxi data (24 thÃ¡ng)
- âœ… Upload lÃªn HDFS
- âœ… Verify data integrity

### Tuáº§n 4-5: Development
- âœ… Viáº¿t code build graph
- âœ… Viáº¿t code PageRank
- âœ… Viáº¿t code clustering
- âœ… Test trÃªn sample data

### Tuáº§n 6-7: Cháº¡y full pipeline
- âœ… Run build graph trÃªn full data
- âœ… Run PageRank
- âœ… Run clustering
- âœ… Generate visualizations

### Tuáº§n 8: Benchmark
- âœ… Cháº¡y benchmark tests
- âœ… Thu tháº­p metrics
- âœ… PhÃ¢n tÃ­ch scalability

### Tuáº§n 9-10: Viáº¿t bÃ¡o cÃ¡o
- âœ… Viáº¿t report (LaTeX/Word)
- âœ… Táº¡o slides presentation
- âœ… Chuáº©n bá»‹ demo

### Tuáº§n 11-12: HoÃ n thiá»‡n vÃ  báº£o vá»‡
- âœ… Review vÃ  polish
- âœ… Practice presentation
- âœ… Báº£o vá»‡ Ä‘á»“ Ã¡n

---

## ğŸ¯ CÃC FILE QUAN TRá»ŒNG VÃ€ Má»¤C ÄÃCH

| File | Má»¥c Ä‘Ã­ch | Khi nÃ o dÃ¹ng |
|------|----------|--------------|
| **README.md** | Tá»•ng quan project, architecture, usage | Äá»c Ä‘áº§u tiÃªn Ä‘á»ƒ hiá»ƒu project |
| **QUICKSTART.md** | HÆ°á»›ng dáº«n nhanh, TL;DR | Cáº§n setup vÃ  cháº¡y nhanh |
| **setup_guide.md** | HÆ°á»›ng dáº«n cÃ i Ä‘áº·t chi tiáº¿t tá»«ng bÆ°á»›c | Setup láº§n Ä‘áº§u, troubleshooting |
| **PRESENTATION_GUIDE.md** | Cáº¥u trÃºc bÃ¡o cÃ¡o, slides, Q&A | Viáº¿t bÃ¡o cÃ¡o, chuáº©n bá»‹ báº£o vá»‡ |
| **PROJECT_CHECKLIST.md** | Checklist tá»«ng bÆ°á»›c thá»±c hiá»‡n | Tracking progress, Ä‘áº£m báº£o khÃ´ng miss bÆ°á»›c |
| **run_all.sh** | Script tá»± Ä‘á»™ng cháº¡y pipeline | Cháº¡y toÃ n bá»™ 5 bÆ°á»›c cÃ¹ng lÃºc |
| **check_setup.sh** | Kiá»ƒm tra há»‡ thá»‘ng sáºµn sÃ ng | TrÆ°á»›c khi cháº¡y pipeline |

---

## ğŸ”‘ Lá»†NH QUAN TRá»ŒNG Cáº¦N NHá»š

### Quáº£n lÃ½ Hadoop/Spark

```bash
# Start services
start-dfs.sh              # Start HDFS
start-yarn.sh             # Start YARN
start-master.sh           # Start Spark Master
start-workers.sh          # Start Spark Workers

# Stop services
stop-dfs.sh
stop-yarn.sh
stop-master.sh
stop-workers.sh

# Stop all
stop-all.sh

# Check running processes
jps                       # Java processes (NameNode, DataNode, Master, Worker...)
```

### HDFS Commands

```bash
# Basic operations
hdfs dfs -ls /                          # List root
hdfs dfs -ls /user/taxi/raw_data/      # List data
hdfs dfs -mkdir -p /path/to/dir        # Create directory
hdfs dfs -put localfile /hdfs/path     # Upload file
hdfs dfs -get /hdfs/path localpath     # Download file
hdfs dfs -rm -r /hdfs/path             # Delete

# Check health
hdfs dfsadmin -report                  # Cluster report
hdfs dfs -du -h /user/taxi/            # Disk usage
```

### Spark Submit

```bash
# Standard command
spark-submit \
  --master spark://master:7077 \
  --executor-memory 2g \
  --driver-memory 2g \
  --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
  path/to/script.py

# With more configs
spark-submit \
  --master spark://master:7077 \
  --executor-memory 2g \
  --driver-memory 2g \
  --executor-cores 2 \
  --num-executors 2 \
  --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
  --conf spark.sql.shuffle.partitions=200 \
  path/to/script.py
```

### Monitor & Debug

```bash
# View logs
tail -f $SPARK_HOME/logs/spark-*.out
tail -f $HADOOP_HOME/logs/hadoop-*-namenode-*.log

# Web UIs
http://master:9870    # HDFS NameNode
http://master:8080    # Spark Master
http://master:8088    # YARN ResourceManager
http://master:4040    # Spark Application (when running)

# System resources
htop                  # CPU & RAM
df -h                 # Disk space
```

---

## ğŸ› TROUBLESHOOTING QUICK REFERENCE

### Lá»—i: Out of Memory

**Triá»‡u chá»©ng:**
```
java.lang.OutOfMemoryError: Java heap space
```

**Giáº£i phÃ¡p:**
1. Giáº£m executor memory: `--executor-memory 1g`
2. TÄƒng partitions trong code: `df.repartition(400)`
3. TÄƒng RAM cho VMs (shutdown â†’ settings â†’ memory)

---

### Lá»—i: Connection Refused (HDFS)

**Triá»‡u chá»©ng:**
```
Call From master/192.168.x.x to master:9000 failed on connection exception
```

**Giáº£i phÃ¡p:**
```bash
# Kiá»ƒm tra HDFS running
jps | grep NameNode

# Náº¿u khÃ´ng tháº¥y, restart
stop-dfs.sh
start-dfs.sh

# Check Web UI
curl http://master:9870
```

---

### Lá»—i: Spark Workers not connecting

**Triá»‡u chá»©ng:**
- Spark Web UI chá»‰ tháº¥y Master, khÃ´ng tháº¥y Workers

**Giáº£i phÃ¡p:**
```bash
# 1. Check /etc/hosts
cat /etc/hosts   # Pháº£i cÃ³ entry cho master, worker1

# 2. Check SSH passwordless
ssh worker1      # KhÃ´ng cáº§n password

# 3. Restart Spark
stop-workers.sh
stop-master.sh
start-master.sh
start-workers.sh
```

---

### Lá»—i: GraphFrames not found

**Triá»‡u chá»©ng:**
```
Py4JJavaError: ... graphframes not found
```

**Giáº£i phÃ¡p:**
```bash
# Option 1: Use --packages
spark-submit --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 ...

# Option 2: Download JAR manually
wget https://repos.spark-packages.org/.../graphframes-0.8.3-spark3.5-s_2.12.jar
sudo cp *.jar $SPARK_HOME/jars/
```

---

### Lá»—i: Data not found in HDFS

**Triá»‡u chá»©ng:**
```
Path does not exist: /user/taxi/raw_data
```

**Giáº£i phÃ¡p:**
```bash
# Check path exists
hdfs dfs -ls /user/taxi/

# Create if not exists
hdfs dfs -mkdir -p /user/taxi/raw_data

# Upload data
hdfs dfs -put ~/nyc_taxi_data/*.parquet /user/taxi/raw_data/

# Verify
hdfs dfs -ls /user/taxi/raw_data/ | wc -l
```

---

## ğŸ“Š EXPECTED RESULTS - Káº¾T QUáº¢ CHUáº¨N

### Edge List
```
Total edges: 50-100 million
Example top edges:
  Zone 237 â†’ 236: 8,500,000 trips
  Zone 236 â†’ 237: 8,200,000 trips
  Zone 161 â†’ 237: 5,100,000 trips
```

### PageRank
```
Top 10 zones (expected PageRank ~ 0.04 - 0.10):
  1. Zone 237 (Manhattan Upper East)
  2. Zone 236 (Manhattan Upper West)
  3. Zone 161 (Manhattan Midtown)
  4. Zone 230 (Times Square area)
  5. Zone 132 (JFK Airport)
  ...
  
Distribution: Power-law
  - Top 10: ~45% total PageRank
  - Top 20: ~65% total PageRank
```

### Communities
```
Number of communities: 25-35
Top 5 largest:
  1. Community 1: 40-50 zones (Manhattan CBD)
  2. Community 2: 30-40 zones (Brooklyn)
  3. Community 3: 25-35 zones (Queens + airports)
  4. Community 4: 20-25 zones (Bronx)
  5. Community 5: 15-20 zones (Staten Island)
```

### Benchmark
```
Build Graph:
  1M rows:   25-45 seconds (2 nodes)
  10M rows:  400-700 seconds (2 nodes)
  Full data: 1-2 hours (2 nodes)
  
PageRank (20 iterations):
  25-60 minutes (2 nodes)
  
Speedup (2 nodes vs 1 node):
  ~1.5-2.0x
```

---

## ğŸ“ TEMPLATES & EXAMPLES

### LaTeX cÃ´ng thá»©c PageRank

```latex
\begin{equation}
PR(i) = \frac{1-d}{|V|} + d \sum_{j \in In(i)} \frac{PR(j)}{outdeg(j)}
\end{equation}

where:
\begin{itemize}
  \item $d = 0.85$ (damping factor)
  \item $In(i)$ = set of nodes with edge to $i$
  \item $outdeg(j)$ = out-degree of node $j$
  \item $|V|$ = total number of nodes
\end{itemize}
```

### Table cho bÃ¡o cÃ¡o

```markdown
| Metric | Value |
|--------|-------|
| Dataset Size | ~30 GB |
| Number of Trips | 200-300 million |
| Number of Zones | 260 |
| Number of Edges | 50-100 million |
| PageRank Iterations | 20 |
| Clustering Algorithm | Label Propagation |
| Cluster Size | 2 nodes (10GB RAM) |
```

---

## ğŸ“ Há»ŒC Tá»ª Dá»° ÃN NÃ€Y

### Ká»¹ nÄƒng há»c Ä‘Æ°á»£c:

âœ… **Distributed Systems:**
- Setup Hadoop/Spark cluster
- HDFS operations
- Distributed computing concepts

âœ… **Big Data Processing:**
- MapReduce paradigm
- PySpark DataFrame operations
- Memory management vá»›i large datasets

âœ… **Graph Mining:**
- PageRank algorithm
- Community detection
- Graph analysis techniques

âœ… **System Administration:**
- Linux VM management
- Network configuration
- Service monitoring

âœ… **Software Engineering:**
- Project structure
- Code organization
- Documentation

---

## ğŸ”— RESOURCES Há»®U ÃCH

### Documentation
- [Spark Documentation](https://spark.apache.org/docs/latest/)
- [GraphFrames Guide](https://graphframes.github.io/graphframes/docs/_site/index.html)
- [Hadoop HDFS Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)

### Learning Resources
- [Mining of Massive Datasets Book](http://www.mmds.org/)
- [CS246 Course (Stanford)](http://web.stanford.edu/class/cs246/)
- [PySpark Tutorial](https://spark.apache.org/docs/latest/api/python/)

### NYC Taxi Data
- [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
- [Taxi Zone Maps](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

---

## âœ‰ï¸ CONTACT & SUPPORT

**Náº¿u gáº·p váº¥n Ä‘á»:**

1. Xem `setup_guide.md` cho troubleshooting
2. Check logs: `$SPARK_HOME/logs/` vÃ  `$HADOOP_HOME/logs/`
3. Google error message + "spark" hoáº·c "hadoop"
4. Stack Overflow: [apache-spark] tag

---

## ğŸ“œ LICENSE & CREDITS

**Project Type:** Educational (MMDS Course Project)

**Credits:**
- NYC TLC for open data
- Apache Spark & Hadoop communities
- CS246 course materials
- GraphFrames library developers

---

**Good luck vá»›i dá»± Ã¡n! ğŸ‰ğŸš€**

---

*Last updated: 2026-02-04*
