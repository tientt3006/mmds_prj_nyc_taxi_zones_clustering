# H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t NYC Taxi Graph Mining v·ªõi Docker

## M·ª§C L·ª§C
1. [Y√™u c·∫ßu h·ªá th·ªëng](#y√™u-c·∫ßu-h·ªá-th·ªëng)
2. [C√†i ƒë·∫∑t Docker](#c√†i-ƒë·∫∑t-docker)
3. [Build v√† kh·ªüi ƒë·ªông cluster](#build-v√†-kh·ªüi-ƒë·ªông-cluster)
4. [Ki·ªÉm tra h·ªá th·ªëng](#ki·ªÉm-tra-h·ªá-th·ªëng)
5. [Upload d·ªØ li·ªáu](#upload-d·ªØ-li·ªáu)
6. [Ch·∫°y code ph√¢n t√≠ch](#ch·∫°y-code-ph√¢n-t√≠ch)
7. [Troubleshooting](#troubleshooting)

---

## Y√äU C·∫¶U H·ªÜ TH·ªêNG

### Hardware t·ªëi thi·ªÉu
- **RAM:** 8GB (khuy·∫øn ngh·ªã 16GB)
- **CPU:** 4 cores
- **Disk:** 50GB tr·ªëng
- **OS:** Windows 10/11, macOS, ho·∫∑c Linux

### Software c·∫ßn c√†i ƒë·∫∑t
- **Docker Desktop** (Windows/Mac) ho·∫∑c **Docker Engine** (Linux)
- **Docker Compose** (th∆∞·ªùng ƒëi k√®m Docker Desktop)

---

## C√ÄI ƒê·∫∂T DOCKER

### Windows 10/11

```powershell
# 1. Download Docker Desktop
# Truy c·∫≠p: https://www.docker.com/products/docker-desktop/

# 2. C√†i ƒë·∫∑t v√† kh·ªüi ƒë·ªông Docker Desktop

# 3. Ki·ªÉm tra c√†i ƒë·∫∑t
docker --version
docker-compose --version

# 4. C·∫•u h√¨nh t√†i nguy√™n
# Docker Desktop > Settings > Resources
# - CPUs: 4
# - Memory: 8GB (ho·∫∑c 12GB n·∫øu c√≥)
# - Swap: 2GB
# - Disk size: 50GB
```

### macOS

```bash
# Download Docker Desktop for Mac
# https://www.docker.com/products/docker-desktop/

# Sau khi c√†i, ki·ªÉm tra
docker --version
docker-compose --version
```

### Linux (Ubuntu/Debian)

```bash
# C√†i Docker Engine
sudo apt update
sudo apt install -y apt-transport-https ca-certificates curl software-properties-common

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# C√†i Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# Th√™m user v√†o group docker
sudo usermod -aG docker $USER
newgrp docker

# Ki·ªÉm tra
docker --version
docker-compose --version
```

---

## BUILD V√Ä KH·ªûI ƒê·ªòNG CLUSTER

### 1. Clone project

```bash
cd d:\neit_ng\prjs_i\py_cmm_4thy2nds\massive_data_minning
```

### 2. Build Docker images

```bash
cd docker

# Build images (l·∫ßn ƒë·∫ßu ti√™n s·∫Ω m·∫•t 5-10 ph√∫t)
docker-compose build

# Xem images ƒë√£ build
docker images | grep taxi-mining
```

### 3. Kh·ªüi ƒë·ªông cluster

```bash
# Kh·ªüi ƒë·ªông t·∫•t c·∫£ services (detached mode)
docker-compose up -d

# Xem logs
docker-compose logs -f

# Xem logs c·ªßa t·ª´ng service
docker-compose logs -f master
docker-compose logs -f worker1
```

### 4. Ki·ªÉm tra containers ƒëang ch·∫°y

```bash
# List containers
docker-compose ps

# K·∫øt qu·∫£ mong ƒë·ª£i:
# NAME                  STATUS         PORTS
# taxi-mining-master    Up 2 minutes   0.0.0.0:8088->8088/tcp, ...
# taxi-mining-worker1   Up 2 minutes   
```

---

## KI·ªÇM TRA H·ªÜ TH·ªêNG

### 1. Ki·ªÉm tra Web UIs

M·ªü tr√¨nh duy·ªát v√† truy c·∫≠p:

| Service | URL | M√¥ t·∫£ |
|---------|-----|-------|
| **HDFS NameNode** | http://localhost:9870 | Qu·∫£n l√Ω HDFS |
| **YARN ResourceManager** | http://localhost:8088 | Qu·∫£n l√Ω jobs |
| **Spark Master** | http://localhost:8080 | Spark cluster UI |
| **Spark History Server** | http://localhost:18080 | L·ªãch s·ª≠ Spark jobs |

### 2. Ki·ªÉm tra HDFS

```bash
# V√†o container master
docker exec -it taxi-mining-master bash

# Test HDFS
hdfs dfs -mkdir /test
echo "Hello HDFS" > /tmp/test.txt
hdfs dfs -put /tmp/test.txt /test/
hdfs dfs -cat /test/test.txt
hdfs dfs -rm -r /test

# Tho√°t container
exit
```

### 3. Ki·ªÉm tra Spark

```bash
# V√†o container master
docker exec -it taxi-mining-master bash

# Test Spark job (t√≠nh Pi)
spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master spark://master:7077 \
    $SPARK_HOME/examples/jars/spark-examples_*.jar 100

# PySpark shell
pyspark --master spark://master:7077

# Trong PySpark shell:
>>> rdd = sc.parallelize(range(100))
>>> print(rdd.sum())
>>> exit()

# Tho√°t container
exit
```

---

## UPLOAD D·ªÆ LI·ªÜU

### 1. Download d·ªØ li·ªáu NYC Taxi

```bash
# Tr√™n m√°y host (Windows/Mac/Linux)
cd d:\neit_ng\prjs_i\py_cmm_4thy2nds\massive_data_minning

# T·∫°o th∆∞ m·ª•c data
mkdir -p data/raw

# Download sample data (th√°ng 1/2019)
# Windows (PowerShell)
Invoke-WebRequest -Uri "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet" -OutFile "data/raw/yellow_tripdata_2019-01.parquet"

# Linux/Mac
cd data/raw
wget "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet"

# Download zone lookup
wget "https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"
```

### 2. Copy d·ªØ li·ªáu v√†o container

```bash
# Copy t·ª´ host v√†o container master
docker cp data/raw/yellow_tripdata_2019-01.parquet taxi-mining-master:/tmp/
docker cp data/raw/taxi+_zone_lookup.csv taxi-mining-master:/tmp/
```

### 3. Upload l√™n HDFS

```bash
# V√†o container master
docker exec -it taxi-mining-master bash

# T·∫°o th∆∞ m·ª•c HDFS
hdfs dfs -mkdir -p /user/taxi/raw_data
hdfs dfs -mkdir -p /user/taxi/zone_lookup

# Upload d·ªØ li·ªáu
hdfs dfs -put /tmp/yellow_tripdata_2019-01.parquet /user/taxi/raw_data/
hdfs dfs -put /tmp/taxi+_zone_lookup.csv /user/taxi/zone_lookup/

# Ki·ªÉm tra
hdfs dfs -ls /user/taxi/raw_data/
hdfs dfs -ls /user/taxi/zone_lookup/

# Xem th√¥ng tin chi ti·∫øt
hdfs fsck /user/taxi/raw_data/ -files -blocks -locations

# Tho√°t container
exit
```

---

## CH·∫†Y CODE PH√ÇN T√çCH

### 1. Copy code v√†o container

```bash
# T·ª´ th∆∞ m·ª•c project root
docker cp src/ taxi-mining-master:/workspace/
docker cp notebooks/ taxi-mining-master:/workspace/
docker cp config/ taxi-mining-master:/workspace/
```

### 2. Ch·∫°y script Python

```bash
# V√†o container master
docker exec -it taxi-mining-master bash

# Navigate to workspace
cd /workspace

# Ch·∫°y script build graph
spark-submit \
    --master spark://master:7077 \
    --deploy-mode client \
    --driver-memory 2g \
    --executor-memory 2g \
    --executor-cores 2 \
    --num-executors 2 \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/1_build_graph.py

# Ch·∫°y PageRank
spark-submit \
    --master spark://master:7077 \
    --deploy-mode client \
    --driver-memory 2g \
    --executor-memory 2g \
    --packages graphframes:graphframes:0.8.3-spark3.5-s_2.12 \
    src/2_pagerank.py

# Tho√°t container
exit
```

### 3. S·ª≠ d·ª•ng Jupyter Notebook

```bash
# Kh·ªüi ƒë·ªông Jupyter trong container
docker exec -it taxi-mining-master bash

# Start Jupyter
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root

# Copy token t·ª´ output, v√≠ d·ª•:
# http://127.0.0.1:8888/?token=abc123...

# M·ªü tr√¨nh duy·ªát tr√™n m√°y host:
# http://localhost:8888/?token=abc123...
```

### 4. L·∫•y k·∫øt qu·∫£ v·ªÅ m√°y host

```bash
# Copy results t·ª´ container ra host
docker cp taxi-mining-master:/workspace/results/ ./results/

# Ho·∫∑c t·ª´ HDFS
docker exec taxi-mining-master hdfs dfs -get /user/taxi/results/ /tmp/results
docker cp taxi-mining-master:/tmp/results ./results/
```

---

## QU·∫¢N L√ù CLUSTER

### D·ª´ng cluster

```bash
# D·ª´ng t·∫•t c·∫£ containers (gi·ªØ l·∫°i d·ªØ li·ªáu)
docker-compose stop

# D·ª´ng v√† x√≥a containers (gi·ªØ volumes)
docker-compose down

# D·ª´ng v√† x√≥a containers + volumes (M·∫§T D·ªÆ LI·ªÜU)
docker-compose down -v
```

### Kh·ªüi ƒë·ªông l·∫°i cluster

```bash
# Kh·ªüi ƒë·ªông l·∫°i t·ª´ tr·∫°ng th√°i stopped
docker-compose start

# Ho·∫∑c kh·ªüi ƒë·ªông m·ªõi ho√†n to√†n
docker-compose up -d
```

### Xem logs

```bash
# Logs t·∫•t c·∫£ services
docker-compose logs -f

# Logs service c·ª• th·ªÉ
docker-compose logs -f master
docker-compose logs -f worker1

# Logs 100 d√≤ng cu·ªëi
docker-compose logs --tail=100 master
```

### Scale workers

```bash
# Th√™m workers (t·ªëi ƒëa 3 nh∆∞ c·∫•u h√¨nh)
docker-compose up -d --scale worker=3

# Gi·∫£m xu·ªëng 1 worker
docker-compose up -d --scale worker=1
```

### V√†o container ƒë·ªÉ debug

```bash
# V√†o master
docker exec -it taxi-mining-master bash

# V√†o worker1
docker exec -it taxi-mining-worker1 bash

# Ch·∫°y l·ªánh tr·ª±c ti·∫øp kh√¥ng v√†o shell
docker exec taxi-mining-master hdfs dfs -ls /
```

---

## TROUBLESHOOTING

### 1. Container kh√¥ng kh·ªüi ƒë·ªông

```bash
# Xem logs chi ti·∫øt
docker-compose logs master

# Ki·ªÉm tra t√†i nguy√™n
docker stats

# Restart container
docker-compose restart master
```

### 2. HDFS kh√¥ng ho·∫°t ƒë·ªông

```bash
# V√†o container master
docker exec -it taxi-mining-master bash

# Ki·ªÉm tra HDFS processes
jps
# N√™n th·∫•y: NameNode, DataNode, SecondaryNameNode

# Format l·∫°i NameNode (CH·ªà khi c·∫ßn thi·∫øt - M·∫§T D·ªÆ LI·ªÜU)
hdfs namenode -format -force

# Restart HDFS
stop-dfs.sh
start-dfs.sh
```

### 3. Spark job b·ªã fail

```bash
# Xem logs Spark
docker exec taxi-mining-master tail -f $SPARK_HOME/logs/*

# Xem Spark UI
# http://localhost:8080

# Ki·ªÉm tra workers
docker exec taxi-mining-master cat $SPARK_HOME/conf/workers
```

### 4. Out of Memory

```bash
# TƒÉng memory cho container trong docker-compose.yml
# S·ª≠a ph·∫ßn:
    deploy:
      resources:
        limits:
          memory: 4G  # TƒÉng t·ª´ 3G l√™n 4G

# Ho·∫∑c gi·∫£m executor memory trong spark-submit
spark-submit \
    --driver-memory 1g \
    --executor-memory 1g \
    ...
```

### 5. Port ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng

```bash
# Windows: T√¨m process s·ª≠ d·ª•ng port
netstat -ano | findstr :9870
taskkill /PID <PID> /F

# Linux/Mac
lsof -i :9870
kill -9 <PID>

# Ho·∫∑c ƒë·ªïi port trong docker-compose.yml
```

### 6. Clean up to√†n b·ªô

```bash
# D·ª´ng v√† x√≥a containers
docker-compose down -v

# X√≥a images
docker rmi $(docker images -q 'taxi-mining*')

# X√≥a volumes
docker volume prune -f

# X√≥a t·∫•t c·∫£
docker system prune -a --volumes -f
```

---

## MONITORING V√Ä PERFORMANCE

### 1. Theo d√µi t√†i nguy√™n

```bash
# Real-time stats
docker stats

# Top processes trong container
docker exec taxi-mining-master top
```

### 2. Ki·ªÉm tra HDFS health

```bash
docker exec taxi-mining-master hdfs dfsadmin -report
docker exec taxi-mining-master hdfs fsck / -files -blocks -locations
```

### 3. Ki·ªÉm tra Spark cluster

```bash
# Spark Master UI: http://localhost:8080
# YARN UI: http://localhost:8088
# Application UI: http://localhost:4040 (khi job ch·∫°y)
```

---

## BEST PRACTICES

### 1. Backup d·ªØ li·ªáu

```bash
# Backup HDFS
docker exec taxi-mining-master hdfs dfs -get /user/taxi /tmp/backup
docker cp taxi-mining-master:/tmp/backup ./backup/

# Restore
docker cp ./backup/ taxi-mining-master:/tmp/
docker exec taxi-mining-master hdfs dfs -put /tmp/backup/* /user/taxi/
```

### 2. L∆∞u k·∫øt qu·∫£

```bash
# Lu√¥n copy results ra host
docker cp taxi-mining-master:/workspace/results ./results/

# Ho·∫∑c mount volume (th√™m v√†o docker-compose.yml)
volumes:
  - ./results:/workspace/results
```

### 3. Development workflow

```bash
# 1. Vi·∫øt code tr√™n host (VSCode, PyCharm, etc.)
# 2. Copy v√†o container
docker cp src/new_script.py taxi-mining-master:/workspace/src/

# 3. Test
docker exec taxi-mining-master spark-submit /workspace/src/new_script.py

# 4. L·∫•y k·∫øt qu·∫£
docker cp taxi-mining-master:/workspace/results ./results/
```

---

## T√ÄI LI·ªÜU THAM KH·∫¢O

- [Docker Documentation](https://docs.docker.com/)
- [Hadoop Documentation](https://hadoop.apache.org/docs/stable/)
- [Spark Documentation](https://spark.apache.org/docs/latest/)
- [GraphFrames Guide](https://graphframes.github.io/graphframes/docs/_site/user-guide.html)

---

**üéâ HO√ÄN TH√ÄNH! Cluster ƒë√£ s·∫µn s√†ng ƒë·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu NYC Taxi.**
